{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship Between Energy Consumption and Social/Economic Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Motivation**\n",
    "In this project, we will investigate the relationship between energy consumption and social/economic measures. Climate change is ultimately a global issue, so we will use data from as many countries all over the globe as possible. Thus, we will have data at a country level about energy consumption and about social/economic measures. Everything is on a yearly level, spanning from 1965 to 2020. We have the data from Our world in Data (source: ourworldindata.org). For the energy data, we will use per capita consumption, and we will look at: coal, fossil fuels aggregated, gas, hydro, low carbon aggregated, nuclear, oil, other renewables (residual category), solar, wind, and total. For the social and economic data, we will look at: child mortality, GDP, GDP per capita, Human Development Index (HDI), life expectancy, literacy rate, military spending, and obesity. \n",
    "\n",
    "In terms of choice of data to tackle the problem with, the energy data is pretty straight-forward. It's a bit trickier with the social/economic measures, and how best to measure things is itself an ongoing debate. We decided to base our choices mostly on this source: https://www.tutor2u.net/geography/reference/the-8-key-gap-indicators-of-development, and then added some of our own. We believe this is a relatively diverse representation of a country's social and economic state. \n",
    "\n",
    "Ultimately, the goal of the project is to give people insight into how social and economic differences between countries and continents influences energy consumption patterns. Climate change is one of the large problems that society is facing, and the fact that we have to think about the problem as a global phenomenon makes it tricky. The aim is therefore to serve information about the topic in an intuitive and interesting way, which will then allow people to have informed opinions about the matter and ultimately make better decisions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Basic stats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing\n",
    "The first step is data cleaning and preprocessing. In this case, there is quite a lot. Both due to the fact that we use a large number of raw datasets, and that we have a large number of missing values. This latter point can be due to a whole host of reasons. First of all, some countries only start existing after 1965. Additionally, even though a country exists, it is not certain that the measurements have been made. This is particularly true for the African countries. Additionally, there missing values even after recording from a certain country has started. In order to enrich the data further, we will make use of linear interpolation and constant extrapolation (see respective section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import os\n",
    "from functools import reduce\n",
    "from country_list import countries_for_language\n",
    "import pycountry_convert as pc\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load and join raw data**\n",
    "\n",
    "Load and join raw data, both energy and social data.\n",
    "\n",
    "- ***General comments***: From the webside \"ourworldindata.org\" we could see that data from 2020 was not completed so we discarded all data from 2020 to preserve accuracy of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to raw data files\n",
    "in_path = os.path.join(os.getcwd(), \"../data/data_for_notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and join energy consumption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging ac_dfs (12)\n",
      "Merging pcc_dfs (11)\n"
     ]
    }
   ],
   "source": [
    "# Get energy consumption files\n",
    "energy_data = [x for x in os.listdir(os.path.join(in_path,\"EnergyData\"))]\n",
    "\n",
    "# Annual and per capita consumption paths\n",
    "ac_paths = list(filter(lambda x: \"ac\" in x.lower(), energy_data))\n",
    "pcc_paths = list(filter(lambda x: \"pcc\" in x.lower(), energy_data))\n",
    "\n",
    "print(f\"Merging ac_dfs ({len(ac_paths)})\")\n",
    "ac_dfs = [\n",
    "    pd.read_csv(os.path.join(in_path, \"EnergyData\", filename))\n",
    "    for filename in ac_paths\n",
    "]\n",
    "for df in ac_dfs:\n",
    "    df.drop(\"Code\", inplace=True, axis=1)\n",
    "ac_final = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=[\"Entity\", \"Year\"], how=\"outer\"),\n",
    "    ac_dfs,\n",
    ")\n",
    "\n",
    "print(f\"Merging pcc_dfs ({len(pcc_paths)})\")\n",
    "pcc_dfs = [\n",
    "    pd.read_csv(os.path.join(in_path, \"EnergyData\", filename))\n",
    "    for filename in pcc_paths\n",
    "]\n",
    "for df in pcc_dfs:\n",
    "    df.drop(\"Code\", inplace=True, axis=1)\n",
    "pcc_final = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=[\"Entity\", \"Year\"], how=\"outer\"),\n",
    "    pcc_dfs,\n",
    ")\n",
    "\n",
    "# Annual consumption joined data\n",
    "ac_joined = ac_final[(ac_final.Year >= 1965) & (ac_final.Year < 2020)].reset_index().drop(columns=\"index\")\n",
    "# Per capita consumption joined data\n",
    "pcc_joined = pcc_final[(pcc_final.Year >= 1965) & (pcc_final.Year < 2020)].reset_index().drop(columns=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Coal per capita (kWh)</th>\n",
       "      <th>Fossil Fuels per capita (kWh)</th>\n",
       "      <th>Energy per capita (kWh)</th>\n",
       "      <th>Low-carbon energy per capita (kWh)</th>\n",
       "      <th>Gas per capita (kWh)</th>\n",
       "      <th>Nuclear per capita (kWh)</th>\n",
       "      <th>Oil per capita (kWh)</th>\n",
       "      <th>Renewables per capita (kWh)</th>\n",
       "      <th>Wind per capita (kWh)</th>\n",
       "      <th>Solar per capita (kWh)</th>\n",
       "      <th>Hydro per capita (kWh)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Africa</td>\n",
       "      <td>1965</td>\n",
       "      <td>1010.461</td>\n",
       "      <td>2062.718</td>\n",
       "      <td>2183.369</td>\n",
       "      <td>120.653</td>\n",
       "      <td>29.811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1022.446</td>\n",
       "      <td>0.120653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Africa</td>\n",
       "      <td>1966</td>\n",
       "      <td>984.359</td>\n",
       "      <td>2107.879</td>\n",
       "      <td>2239.127</td>\n",
       "      <td>131.249</td>\n",
       "      <td>32.505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1091.015</td>\n",
       "      <td>0.131249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Africa</td>\n",
       "      <td>1967</td>\n",
       "      <td>981.168</td>\n",
       "      <td>2067.899</td>\n",
       "      <td>2201.498</td>\n",
       "      <td>133.600</td>\n",
       "      <td>31.327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1055.404</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Africa</td>\n",
       "      <td>1968</td>\n",
       "      <td>994.882</td>\n",
       "      <td>2111.486</td>\n",
       "      <td>2263.843</td>\n",
       "      <td>152.359</td>\n",
       "      <td>30.957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1085.647</td>\n",
       "      <td>0.152359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Africa</td>\n",
       "      <td>1969</td>\n",
       "      <td>978.624</td>\n",
       "      <td>2086.611</td>\n",
       "      <td>2259.926</td>\n",
       "      <td>173.317</td>\n",
       "      <td>35.267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1072.720</td>\n",
       "      <td>0.173317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Entity  Year  Coal per capita (kWh)  Fossil Fuels per capita (kWh)  \\\n",
       "0  Africa  1965               1010.461                       2062.718   \n",
       "1  Africa  1966                984.359                       2107.879   \n",
       "2  Africa  1967                981.168                       2067.899   \n",
       "3  Africa  1968                994.882                       2111.486   \n",
       "4  Africa  1969                978.624                       2086.611   \n",
       "\n",
       "   Energy per capita (kWh)  Low-carbon energy per capita (kWh)  \\\n",
       "0                 2183.369                             120.653   \n",
       "1                 2239.127                             131.249   \n",
       "2                 2201.498                             133.600   \n",
       "3                 2263.843                             152.359   \n",
       "4                 2259.926                             173.317   \n",
       "\n",
       "   Gas per capita (kWh)  Nuclear per capita (kWh)  Oil per capita (kWh)  \\\n",
       "0                29.811                       0.0              1022.446   \n",
       "1                32.505                       0.0              1091.015   \n",
       "2                31.327                       0.0              1055.404   \n",
       "3                30.957                       0.0              1085.647   \n",
       "4                35.267                       0.0              1072.720   \n",
       "\n",
       "   Renewables per capita (kWh)  Wind per capita (kWh)  Solar per capita (kWh)  \\\n",
       "0                     0.120653                    0.0                     0.0   \n",
       "1                     0.131249                    0.0                     0.0   \n",
       "2                     0.133600                    0.0                     0.0   \n",
       "3                     0.152359                    0.0                     0.0   \n",
       "4                     0.173317                    0.0                     0.0   \n",
       "\n",
       "   Hydro per capita (kWh)  \n",
       "0                 120.653  \n",
       "1                 131.249  \n",
       "2                 133.600  \n",
       "3                 152.359  \n",
       "4                 173.317  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcc_joined.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and join social data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging socio_dfs (12)\n"
     ]
    }
   ],
   "source": [
    "# Get social data files\n",
    "socio_data = [x for x in os.listdir(os.path.join(in_path, \"Socio_eco_data\"))]\n",
    "socio_data = [x for x in socio_data if '.DS_Store' not in x]\n",
    "\n",
    "print(f\"Merging socio_dfs ({len(socio_data)})\")\n",
    "socio_dfs = [\n",
    "    pd.read_csv(os.path.join(in_path, \"Socio_eco_data\", filename))\n",
    "    for filename in socio_data\n",
    "]\n",
    "\n",
    "for df in socio_dfs:\n",
    "    if \"Code\" in df.columns:\n",
    "        df.drop(\"Code\", inplace=True, axis=1)\n",
    "    for col in df.columns:\n",
    "        if \"annotations\" in col:\n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "socio_data_final = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=[\"Entity\", \"Year\"], how=\"outer\"),\n",
    "    socio_dfs,\n",
    ")\n",
    "\n",
    "socio_data_joined = (\n",
    "    socio_data_final.sort_values([\"Entity\", \"Year\"])\n",
    "    .reset_index()\n",
    "    .drop(columns=\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Literacy rates (World Bank, CIA World Factbook, and other sources)</th>\n",
       "      <th>military_expenditure</th>\n",
       "      <th>GDP per capita</th>\n",
       "      <th>Mortality rate, under-5 (per 1,000 live births)</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Population (historical estimates)</th>\n",
       "      <th>Human Development Index (UNDP)</th>\n",
       "      <th>Life expectancy</th>\n",
       "      <th>Indicator:Prevalence of obesity among adults, BMI &amp;GreaterEqual; 30 (crude estimate) (%) - Sex:Both sexes</th>\n",
       "      <th>Gross enrolment ratio, tertiary, both sexes (%)</th>\n",
       "      <th>Individuals using the Internet (% of population)</th>\n",
       "      <th>Total tax revenue (% of GDP) (ICTD (2021))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>-10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14737.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>-9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20405.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>-8000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28253.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>-7000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>-6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54166.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entity   Year  \\\n",
       "0  Afghanistan -10000   \n",
       "1  Afghanistan  -9000   \n",
       "2  Afghanistan  -8000   \n",
       "3  Afghanistan  -7000   \n",
       "4  Afghanistan  -6000   \n",
       "\n",
       "   Literacy rates (World Bank, CIA World Factbook, and other sources)  \\\n",
       "0                                                NaN                    \n",
       "1                                                NaN                    \n",
       "2                                                NaN                    \n",
       "3                                                NaN                    \n",
       "4                                                NaN                    \n",
       "\n",
       "   military_expenditure  GDP per capita  \\\n",
       "0                   NaN             NaN   \n",
       "1                   NaN             NaN   \n",
       "2                   NaN             NaN   \n",
       "3                   NaN             NaN   \n",
       "4                   NaN             NaN   \n",
       "\n",
       "   Mortality rate, under-5 (per 1,000 live births)  GDP  \\\n",
       "0                                              NaN  NaN   \n",
       "1                                              NaN  NaN   \n",
       "2                                              NaN  NaN   \n",
       "3                                              NaN  NaN   \n",
       "4                                              NaN  NaN   \n",
       "\n",
       "   Population (historical estimates)  Human Development Index (UNDP)  \\\n",
       "0                            14737.0                             NaN   \n",
       "1                            20405.0                             NaN   \n",
       "2                            28253.0                             NaN   \n",
       "3                            39120.0                             NaN   \n",
       "4                            54166.0                             NaN   \n",
       "\n",
       "   Life expectancy  \\\n",
       "0              NaN   \n",
       "1              NaN   \n",
       "2              NaN   \n",
       "3              NaN   \n",
       "4              NaN   \n",
       "\n",
       "   Indicator:Prevalence of obesity among adults, BMI &GreaterEqual; 30 (crude estimate) (%) - Sex:Both sexes  \\\n",
       "0                                                NaN                                                           \n",
       "1                                                NaN                                                           \n",
       "2                                                NaN                                                           \n",
       "3                                                NaN                                                           \n",
       "4                                                NaN                                                           \n",
       "\n",
       "   Gross enrolment ratio, tertiary, both sexes (%)  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2                                              NaN   \n",
       "3                                              NaN   \n",
       "4                                              NaN   \n",
       "\n",
       "   Individuals using the Internet (% of population)  \\\n",
       "0                                               NaN   \n",
       "1                                               NaN   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               NaN   \n",
       "\n",
       "   Total tax revenue (% of GDP) (ICTD (2021))  \n",
       "0                                         NaN  \n",
       "1                                         NaN  \n",
       "2                                         NaN  \n",
       "3                                         NaN  \n",
       "4                                         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "socio_data_joined.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simple preprocessing of joined data**\n",
    "\n",
    "Changing column names of social data and remove unwanted columns.\n",
    "\n",
    "- ***General comments***: From the above short outline of the social data dataframe we see that we had a multiple of columns from the raw data, which we did not wish to include in the analysis. These are therefore discarded and the remaining column names are furthermore changed to more suitable names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple preprocessing of social data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data (Column renaming and extraction)\n",
      "Removing all rows with year prior to 1965\n"
     ]
    }
   ],
   "source": [
    "# Column renaming and removal of unwanted columns from social data\n",
    "df = socio_data_joined.copy()\n",
    "\n",
    "print(f\"Preprocessing data (Column renaming and extraction)\")\n",
    "\n",
    "cols = list(df.columns)\n",
    "for indx, col in enumerate(cols):\n",
    "    if \"Mortality rate, under-5\" in col:\n",
    "        cols[indx] = \"Child mortality rate (under 5 years - %)\"\n",
    "        df.loc[:, col] = (1.0 * df.loc[:, col]) / 1000.0\n",
    "    elif \"Human Dev\" in col:\n",
    "        cols[indx] = \"HDI\"\n",
    "    elif \"Access to basic drinking water\" == col:\n",
    "        cols[indx] = \"Basic_Drinking_Water_Rate\"\n",
    "    elif \"Access to basic sanitation services\" == col:\n",
    "        cols[indx] = \"Basic_Sanitation_Services_Rate\"\n",
    "    elif \"GDP per\" in col:\n",
    "        cols[indx] = \"GDP per capita ($)\"\n",
    "    elif \"Life expec\" in col:\n",
    "        cols[indx] = \"Life expectancy (years)\"\n",
    "    elif \"Entity\" in col:\n",
    "        cols[indx] = \"Entity\"\n",
    "    elif \"Year\" in col:\n",
    "        cols[indx] = \"Year\"\n",
    "    elif \"tertiary\" in col:\n",
    "        cols[indx] = \"Tertiary education (%)\"\n",
    "    elif \"Total tax revenue\" in col:\n",
    "        cols[indx] = \"Tax revenue of total GDP (%)\"\n",
    "    elif \"Individuals using the Internet\" in col:\n",
    "        cols[indx] = \"Internet users (%)\"\n",
    "    elif \"Population\" in col:\n",
    "        cols[indx] = \"Population\"\n",
    "    else:\n",
    "        cols[indx] = \"to_drop\"\n",
    "        df.drop(columns=col, inplace=True)\n",
    "\n",
    "df.columns = [col for col in cols if \"to_drop\" not in col]\n",
    "\n",
    "# Adjust for wrong unit for HDI\n",
    "df[\"HDI\"] = pd.to_numeric(df[\"HDI\"])\n",
    "for indx in range(len(df[\"HDI\"])):\n",
    "    if df.loc[indx, \"HDI\"] > 1:\n",
    "        df.loc[indx, \"HDI\"] = df.loc[indx, \"HDI\"] / 1000\n",
    "\n",
    "# Remove data prior to 1965, since we do not have any energy measures prior to this year.\n",
    "print(f\"Removing all rows with year prior to 1965\")\n",
    "df[\"Year\"] = pd.to_numeric(df[\"Year\"])\n",
    "df = df[df.Year >= 1965].reset_index().drop(columns=\"index\")\n",
    "df = df[df.Year < 2020].reset_index().drop(columns=\"index\")\n",
    "\n",
    "\n",
    "socio_data_joined = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fraction of nan values in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of nan values in the social data is currently: 0.368\n",
      "The fraction of nan values in the energy data is currently: 0.424\n"
     ]
    }
   ],
   "source": [
    "nan_frac_social = socio_data_joined.isnull().sum().sum()/(socio_data_joined.shape[0]*socio_data_joined.shape[1])\n",
    "nan_frac_energy = pcc_joined.isnull().sum().sum()/(pcc_joined.shape[0]*pcc_joined.shape[1])\n",
    "\n",
    "print(f'The fraction of nan values in the social data is currently: {nan_frac_social:.3f}')\n",
    "print(f'The fraction of nan values in the energy data is currently: {nan_frac_energy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Interpolating, extrapolating and splitting data on country/area level**\n",
    "\n",
    "*As seen in the above section, the amount of nan-values in our data is quite excessive at this point (about 43% for both datasets). During this section we manage to reduce the amount of non-nan values of the social data by about 7 percent points.*\n",
    "\n",
    "The main idea for this preprocessing step is to enrich our data, in order for us to better model and vizualize the data in a meaningfull way. We are aware that this introduces bias into the dataset, but we did feel like this was a better choice overall. Below is a short description of, how we did interpolate and extrapolate data from known data:\n",
    "\n",
    "\n",
    "- **Interpolating data:** We have done a linear interpolation for all datapoints in-between two known values wihtin the same *entity/country*. This means, that if we had a known value for *HDI* for Japan in 1985, no values for 1986-1992, and a known value for 1993, then we do linear interpolation of the datapoints in the timerange 1986-1992 based on 1985 and 1993.\n",
    "\n",
    "- **Extrapolating data:** Besides linear interpolation of data, we also did simple exterapolation of unknown datapoints. Specifically we choose to assume, that any given observed value would likely be somewhat the same 5 years into the future (and 5 years into the past). That is, we did constant extrapolation of unknown data. We are aware that linear extrapolation or auto-regressive extrapolation would have maybe yield a more fair approximation, but we still feel like the constant assumption was fair, given the few amount of years we did extrapolate.\n",
    "\n",
    "- **Splitting data on country/area level:** The raw data included both area specific and country specific measures. In the last part of this preprocessing steps, we did split the data into a country specific and area specific dataframe and discarded the area specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpolating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Interpolation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation function \n",
    "\n",
    "def interpolate_data(df_joined: pd.DataFrame):\n",
    "    print(f\"Interpolating data (linear)\")\n",
    "\n",
    "    df = df_joined.copy()\n",
    "    # Get shapes\n",
    "    N = df.shape[0]\n",
    "    cols_to_interp = df.columns[2:]\n",
    "\n",
    "    # Replace strings with nan\n",
    "    df.replace(\"nan\", np.nan, inplace=True)\n",
    "\n",
    "    # Force columns to be numeric (non entity columns)\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "    # Make pseudo-dataset for interpolating mask\n",
    "    df_interp_mask = df.copy()\n",
    "    df_interp_mask[cols_to_interp] = df_interp_mask[cols_to_interp] * 0\n",
    "    df_interp_mask.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "    # Go through each row - for missing years add rows and interpolate if possible\n",
    "    for i in range(1, N):\n",
    "        year_diff = df.Year[i] - df.Year[i - 1]\n",
    "        if (year_diff > 1) & (df.Entity[i] == df.Entity[i - 1]):\n",
    "            new_rows = np.array(\n",
    "                [\n",
    "                    [df.Entity[i]] * (year_diff - 1),\n",
    "                    np.arange(df.Year[i - 1] + 1, df.Year[i]),\n",
    "                ]\n",
    "            ).T\n",
    "\n",
    "            new_mask = np.array(\n",
    "                [\n",
    "                    [df.Entity[i]] * (year_diff - 1),\n",
    "                    np.arange(df.Year[i - 1] + 1, df.Year[i]),\n",
    "                ]\n",
    "            ).T\n",
    "\n",
    "            for col in cols_to_interp:\n",
    "                if (np.isnan(df.loc[i, col]) == False) & (\n",
    "                    np.isnan(df.loc[i - 1, col]) == False\n",
    "                ):\n",
    "                    new_rows = np.concatenate(\n",
    "                        [\n",
    "                            new_rows,\n",
    "                            np.linspace(\n",
    "                                df.loc[i - 1, col], df.loc[i, col], year_diff + 1\n",
    "                            )[1:-1].reshape((year_diff - 1, 1)),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    new_mask = np.concatenate(\n",
    "                        [\n",
    "                            new_mask,\n",
    "                            np.repeat(1, year_diff - 1).reshape((year_diff - 1, 1)),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                else:\n",
    "                    new_rows = np.concatenate(\n",
    "                        [\n",
    "                            new_rows,\n",
    "                            np.array(np.repeat(np.nan, year_diff - 1)).reshape(\n",
    "                                (year_diff - 1, 1)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    new_mask = np.concatenate(\n",
    "                        [\n",
    "                            new_mask,\n",
    "                            np.repeat(0, year_diff - 1).reshape((year_diff - 1, 1)),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "\n",
    "            new_rows = pd.DataFrame(new_rows, columns=df.columns)\n",
    "            new_rows.replace(\"nan\", np.nan, inplace=True)\n",
    "            for col in new_rows.columns[1:]:\n",
    "                new_rows[col] = pd.to_numeric(new_rows[col])\n",
    "\n",
    "            new_mask = pd.DataFrame(new_mask, columns=df.columns)\n",
    "            for col in new_mask.columns[1:]:\n",
    "                new_mask[col] = pd.to_numeric(new_mask[col])\n",
    "\n",
    "            df = (\n",
    "                pd.concat([df.iloc[:i], new_rows, df.iloc[i:]], axis=0)\n",
    "                .reset_index()\n",
    "                .drop(columns=\"index\")\n",
    "            )\n",
    "            df_interp_mask = (\n",
    "                pd.concat(\n",
    "                    [df_interp_mask.iloc[:i], new_mask, df_interp_mask.iloc[i:]], axis=0\n",
    "                )\n",
    "                .reset_index()\n",
    "                .drop(columns=\"index\")\n",
    "            )\n",
    "            N = N + year_diff - 1\n",
    "\n",
    "    # Go through each column and interpolate values if possible\n",
    "    for col in cols_to_interp:\n",
    "        for i in range(N - 2):\n",
    "            if not np.isnan(df.loc[i, col]):\n",
    "                indx_old_non_nan = i\n",
    "                while (df.Entity[i + 1] == df.Entity[i]) & (\n",
    "                    np.isnan(df.loc[i + 1, col])\n",
    "                ):\n",
    "                    i = i + 1\n",
    "                    if i == N - 1:\n",
    "                        break\n",
    "                if i == N - 1:\n",
    "                    break\n",
    "                if (df.Entity[i + 1] == df.Entity[i]) & (\n",
    "                    indx_old_non_nan != i\n",
    "                ):  # Non nan value followed by x nan values then non nan value (interpolation possible)\n",
    "                    i = i + 1\n",
    "                    df.loc[(indx_old_non_nan + 1) : (i - 1), col] = np.linspace(\n",
    "                        df.loc[indx_old_non_nan, col],\n",
    "                        df.loc[i, col],\n",
    "                        i - indx_old_non_nan + 1,\n",
    "                    )[1:-1]\n",
    "                    df_interp_mask.loc[\n",
    "                        (indx_old_non_nan + 1) : (i - 1), col\n",
    "                    ] = np.repeat(1, i - indx_old_non_nan - 1)\n",
    "                else:\n",
    "                    i = i + 1\n",
    "                    continue\n",
    "\n",
    "\n",
    "    return df, df_interp_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Interpolate energy and social data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating data (linear)\n",
      "Interpolating data (linear)\n",
      "Number of non-null values in raw joined data (energy pc): 71514\n",
      "Number of non-null values in interpolated data (energy pc): 71568\n",
      "Number of non-null values in raw joined data (social): 104201\n",
      "Number of non-null values in interpolated data (social): 108376\n"
     ]
    }
   ],
   "source": [
    "socio_data_inter, socio_data_inter_mask = interpolate_data(socio_data_joined)\n",
    "pcc_inter, pcc_inter_mask = interpolate_data(pcc_joined)\n",
    "\n",
    "print(f'Number of non-null values in raw joined data (energy pc): {(pcc_joined.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in interpolated data (energy pc): {(pcc_inter.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in raw joined data (social): {(socio_data_joined.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in interpolated data (social): {(socio_data_inter.isnull()==False).sum().sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see an improvement in the amount of data, especially for the social data. This will surely help us build a more stable ML model (although more biased), and more smooth visualizations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extrapolating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Extrapolation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolation function\n",
    "\n",
    "def extrapolate_data(df_inter: pd.DataFrame, df_inter_mask: pd.DataFrame, x_extrap: int):\n",
    "    df = df_inter.copy()\n",
    "    df_extrap_mask = df_inter_mask.copy()\n",
    "    \n",
    "    print(f\"Extrapolating data (max {x_extrap} years)\")\n",
    "\n",
    "    # Get shapes\n",
    "    N = df.shape[0]\n",
    "    cols_to_extrap = df.columns[2:]\n",
    "\n",
    "    if x_extrap >= 1:\n",
    "        for col in cols_to_extrap:\n",
    "            for i in range(1, N):\n",
    "                if (\n",
    "                    np.isnan(df.loc[i - 1, col])\n",
    "                    & (not np.isnan(df.loc[i, col]))\n",
    "                    & (df.loc[i - 1, \"Entity\"] == df.loc[i, \"Entity\"])\n",
    "                ):\n",
    "                    m = 1\n",
    "                    if i - m - 1 >= 0:\n",
    "                        while (\n",
    "                            np.isnan(df.loc[i - m - 1, col])\n",
    "                            & (df.loc[i - m - 1, \"Entity\"] == df.loc[i, \"Entity\"])\n",
    "                            & (m + 1 <= x_extrap)\n",
    "                        ):  # Go back a max of x_extrap years\n",
    "                            m = m + 1\n",
    "                            if i - m == 0:\n",
    "                                break\n",
    "\n",
    "                    df.loc[i - m : i - 1, col] = np.repeat(df.loc[i, col], m)\n",
    "                    df_extrap_mask.loc[i - m : i - 1, col] = np.repeat(2, m)\n",
    "\n",
    "            i = 0\n",
    "            while i < N - 1:\n",
    "                if (\n",
    "                    np.isnan(df.loc[i + 1, col])\n",
    "                    & (not np.isnan(df.loc[i, col]))\n",
    "                    & (df.loc[i + 1, \"Entity\"] == df.loc[i, \"Entity\"])\n",
    "                ):\n",
    "                    m = 1\n",
    "                    if i + m + 1 <= N - 1:\n",
    "                        while (\n",
    "                            np.isnan(df.loc[i + m + 1, col])\n",
    "                            & (df.loc[i + m + 1, \"Entity\"] == df.loc[i, \"Entity\"])\n",
    "                            & (m + 1 <= x_extrap)\n",
    "                        ):  # Go forward a max of x_extrap years\n",
    "                            m = m + 1\n",
    "                            if i + m == N - 1:\n",
    "                                break\n",
    "                    df.loc[i + 1 : i + m, col] = np.repeat(df.loc[i, col], m)\n",
    "                    df_extrap_mask.loc[i + 1 : i + m, col] = np.repeat(2, m)\n",
    "                    i += m + 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    return df, df_extrap_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Extrapolating energy and social data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolating data (max 5 years)\n",
      "Extrapolating data (max 5 years)\n",
      "Number of non-null values in interpolated data (energy pc): 71568\n",
      "Number of non-null values in extrapolated data (energy pc): 71568\n",
      "Number of non-null values in interpolated data (social): 108376\n",
      "Number of non-null values in extrapolated data (social): 114765\n"
     ]
    }
   ],
   "source": [
    "socio_data_extrap, socio_data_extrap_mask = extrapolate_data(socio_data_inter, socio_data_inter_mask, 5)\n",
    "pcc_extrap, pcc_extrap_mask = extrapolate_data(pcc_inter, pcc_inter_mask, 5)\n",
    "\n",
    "print(f'Number of non-null values in interpolated data (energy pc): {(pcc_inter.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in extrapolated data (energy pc): {(pcc_extrap.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in interpolated data (social): {(socio_data_inter.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in extrapolated data (social): {(socio_data_extrap.isnull()==False).sum().sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We also see an improvement in the amount of data here, but only for the social data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fraction of nan values in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of nan values in the social data is currently: 0.320\n",
      "The fraction of nan values in the energy data is currently: 0.425\n"
     ]
    }
   ],
   "source": [
    "nan_frac_social = socio_data_extrap.isnull().sum().sum()/(socio_data_extrap.shape[0]*socio_data_extrap.shape[1])\n",
    "nan_frac_energy = pcc_extrap.isnull().sum().sum()/(pcc_extrap.shape[0]*pcc_extrap.shape[1])\n",
    "\n",
    "print(f'The fraction of nan values in the social data is currently: {nan_frac_social:.3f}')\n",
    "print(f'The fraction of nan values in the energy data is currently: {nan_frac_energy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the social data we can see that we have manage to reduce the amount of nan-values with about 7 percent points!\n",
    "\n",
    "**NB:** Be aware that it acutally seems like the fraction of nan-values for the energy data has become larger (which is also true), but the reason being is, that we were able to interpolate one of the columns in the energy data for a certain number of years, which leads to more nan values in the remaining columns (i.e. we add more rows to the data, with only one column being \"filled\" up with non-nan values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting data on country/area level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Splitting function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data on country and area level\n",
    "\n",
    "def split_data_country_area(df_extrap: pd.DataFrame, df_extrap_mask: pd.DataFrame):\n",
    "    df = df_extrap.copy()\n",
    "    df_mask = df_extrap_mask.copy()\n",
    "\n",
    "    print(f\"Splitting data into countries and areas\")\n",
    "\n",
    "    entities = pd.Series(df.Entity.unique(), dtype=\"string\")\n",
    "    entities.replace({\"&\": \"and\"}, inplace=True, regex=True)\n",
    "    countries = pd.Series(np.array(countries_for_language(\"en\"))[:, 1], dtype=\"string\")\n",
    "    countries.replace({\"&\": \"and\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Congo - Brazzaville\": \"Congo\"}, inplace=True, regex=True)\n",
    "    countries.replace(\n",
    "        {\"Congo - Kinshasa\": \"Democratic Republic of Congo\"}, inplace=True, regex=True\n",
    "    )\n",
    "    countries.replace({\"Côte\": \"Cote\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Curaçao\": \"Curacao\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Czechoslovakia\": \"Czechia\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Faroe Islands\": \"Faeroe Islands\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Hong Kong SAR China\": \"Hong Kong\"}, inplace=True, regex=True)\n",
    "    # countries.replace({\"Micronesia\":\"Micronesia (country)\"},inplace=True, regex=True)\n",
    "    countries.replace({\"Macao SAR China\": \"Macao\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Myanmar \\(Burma\\)\": \"Myanmar\"}, inplace=True, regex=True)\n",
    "    countries.replace(\n",
    "        {\"Palestinian Territories\": \"Palestine\"}, inplace=True, regex=True\n",
    "    )\n",
    "    countries.replace(\n",
    "        {\"São Tomé and Príncipe\": \"Sao Tome and Principe\"}, inplace=True, regex=True\n",
    "    )\n",
    "    countries.replace({\"St\\.\": \"Saint\"}, inplace=True, regex=True)\n",
    "    countries.replace(\n",
    "        {\"Saint Vincent and Grenadines\": \"Saint Vincent and the Grenadines\"},\n",
    "        inplace=True,\n",
    "        regex=True,\n",
    "    )\n",
    "    countries.replace(\n",
    "        {\"Saint Martin\": \"Saint Martin (French part)\"}, inplace=True, regex=True\n",
    "    )\n",
    "    countries.replace({\"Timor-Leste\": \"Timor\"}, inplace=True, regex=True)\n",
    "    countries.replace(\n",
    "        {\"Saint Barthélemy\": \"Saint Barthelemy\"}, inplace=True, regex=True\n",
    "    )\n",
    "    countries.replace(\n",
    "        {\"U\\.S\\. Virgin Islands\": \"United States Virgin Islands\"},\n",
    "        inplace=True,\n",
    "        regex=True,\n",
    "    )\n",
    "    countries.replace({\"Vatican City\": \"Vatican\"}, inplace=True, regex=True)\n",
    "    # countries.replace({\"Wallis and Futuna\":\"Wallis and Futuna Islands\"},inplace=True, regex=True)\n",
    "\n",
    "    idx_country = []\n",
    "    for ent in entities:\n",
    "        if any(ent == country for country in countries):\n",
    "            idx_country.append(True)\n",
    "        else:\n",
    "            idx_country.append(False)\n",
    "    map_country = dict(zip(entities, idx_country))\n",
    "\n",
    "    df_ent = df.Entity.astype(\"string\").replace({\"&\": \"and\"}, regex=True)\n",
    "\n",
    "    df_country = (\n",
    "        df.iloc[np.array(df_ent.map(map_country))].reset_index().drop(columns=\"index\")\n",
    "    )\n",
    "    df_area = (\n",
    "        df.iloc[np.array(df_ent.map(map_country) == False)]\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "    )\n",
    "    df_mask_country = (\n",
    "        df_mask.iloc[np.array(df_ent.map(map_country))]\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "    )\n",
    "    df_mask_area = (\n",
    "        df_mask.iloc[np.array(df_ent.map(map_country) == False)]\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "    )\n",
    "\n",
    "    # Add continent info and remove countries with no continent info\n",
    "    countries_code = pd.Series(np.array(countries_for_language('en'))[:,0],dtype=\"string\")\n",
    "    df_country['Continent'] = np.nan\n",
    "    for indx, ent in enumerate(df_country['Entity']):\n",
    "        try:\n",
    "            cont = pc.convert_continent_code_to_continent_name(\n",
    "            pc.country_alpha2_to_continent_code(countries_code.iloc[list(countries).index(ent)]))\n",
    "            df_country.loc[indx,'Continent'] = cont\n",
    "        except:\n",
    "            continue;\n",
    "\n",
    "    cols = df_country.columns[[0,-1]+list(np.arange(1,len(df_country.columns)-1,1))]\n",
    "    df_country = df_country[cols]\n",
    "    df_country = df_country[df_country['Continent'].notna()]\n",
    "\n",
    "\n",
    "    return df_country, df_mask_country, df_area, df_mask_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into countries and areas\n",
      "Splitting data into countries and areas\n"
     ]
    }
   ],
   "source": [
    "social_data, _, _, _ = split_data_country_area(socio_data_extrap, socio_data_extrap_mask)\n",
    "energy_data, _, _, _ = split_data_country_area(pcc_extrap, pcc_extrap_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset stats and exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we made ues the article/magazine style as our data story type. \n",
    "\n",
    "**Visual narrative:**\n",
    "\n",
    "Visual Structuring:\n",
    "- We have made a lot of use of the Progress Bar / Timebar in order to show to progress of both energy consumption and social/economic measures over time. We are very much interested in seeing how things progress over time, so this is an ideal tool. \n",
    "\n",
    "Highlighting:\n",
    "- We generally make use of color in order to direct the viewer's attention. We have a plot of the world that tracks fraction of renewables across time. The color grading goes from black (fraction of 0) to saturated green (fraction of 1), which should make the plot more intuitive. \n",
    "- We also make use of dropdown menues in order to enable the user to narrow down the information in the plots and focus on the particular things that that user finds interesting. \n",
    "- In general, we make quite a lot of use of motion, since, as mentioned earlier, we want to underpin that things are changing quite a lot over time. For instance, the fraction of renewables vs social/economic metric (the one with the blobs) has a lot of motion in it. \n",
    "\n",
    "Transition Guidance:\n",
    "- Since we decided to use a article/magazine style, we do not have transitions in a regular sense. The user will simply scroll down through the article, so the focus in terms of transition are on the ordering of the visuals and of the information. \n",
    "- Additionally, in order to within or between visual scenes, we use quite a few dropdown menues and check-boxes. This might not be transitions in a regular sense, but it is the closest we have to it. \n",
    "\n",
    "**Narrative Structure:**\n",
    "\n",
    "Ordering:\n",
    "- We make use of a linear ordering, suited to the magazine style. We do, however, also want to let the user do some exploration on their own, so we use dropdown menues and checkboxes. \n",
    "\n",
    "Interactivity:\n",
    "- Most figures have quite a lot of interaction. As mentioned earlier, we have both dropdown and checkboxes, and we also have information that pops up on hover-over. \n",
    "- Using the interactivity in the figures is very intuitive and straight-forward, so tutorials are not necessary. We have generally selected default views that make sense. In general, people are much more used to interaction than they might have been, so hand-holding is not as necessary. \n",
    "\n",
    "Messaging:\n",
    "-  In general, we use a lot of annotations in order to communicate information to the user. A great example is the figure for the PLS model, where one can toggle between the components that each have a specific textbox attached to it. \n",
    "- Since we use the article/magazine style, there is naturally a lot of messaging, which means that we also use tools like intoductory text and Summary / Synthesis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots we have created do a good job of telling the story that we want. The combination of an article with some interactive plots strikes a nice balance between finding a story that we want to tell, while also allowing the user to do some exploration on their own. The plots are in general also quite packed with information, but we think we have done a decent job of presenting that information in a way that makes it digestable. We have also managed to extract meaningful information about the problem, while not over-analyzing and jumping to conclusions that are not justified. At it's core, it is a very complex problem, and our analysis is off course not even close to definitive and complete. \n",
    "\n",
    "Things brings us to some of the short-comings of our creation. We got the impression from the litterature that there are other types of models that are often used for these kinds of problems, but we did not find the time to investiate and understand them. Thus, one avenue for improvement is looking more thoroughly into modelling approaches. One thing that is also clear from our analysis is that there is a lot of individuality from country to country. It can therefore be hard to really draw general conclusions. For example, Brazil does not score highly in most of the social/economic measures, but they have a relatively high fraction of renewables (source: https://www.climatescorecard.org/2021/01/brazil-sources-45-of-its-energy-from-renewables/), which might be contrary to what our initial hypothesis was. On the other hand, we have country like the US, where the opposite is true. This goes to show that there are certain limitations to analyzing this problem quantitatively and that it is important to understand the specific circumstances of a country. One could argue that this could also be encoded, and thus modelled quantitatively, but that would require a much larger and more time-consuming analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95bd0d75bea21310707dbe392546405214aa8536e546a1d55110147e5565e008"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
