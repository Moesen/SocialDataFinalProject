{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship Between Energy Consumption and Social/Economic Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Motivation**\n",
    "In this project, we will investigate the relationship between energy consumption and social/economic measures. Climate change is ultimately a global issue, so we will use data from as many countries all over the globe as possible. Thus, we will have data at a country level about energy consumption and about social/economic measures. Everything is on a yearly level, spanning from 1965 to 2020. We have the data from Our world in Data (source: ourworldindata.org). For the energy data, we will use per capita consumption, and we will look at: coal, fossil fuels aggregated, gas, hydro, low carbon aggregated, nuclear, oil, other renewables (residual category), solar, wind, and total. For the social and economic data, we will look at: child mortality, GDP, GDP per capita, Human Development Index (HDI), life expectancy, literacy rate, military spending, and obesity. \n",
    "\n",
    "In terms of choice of data to tackle the problem with, the energy data is pretty straight-forward. It's a bit trickier with the social/economic measures, and how best to measure things is itself an ongoing debate. We decided to base our choices mostly on this source: https://www.tutor2u.net/geography/reference/the-8-key-gap-indicators-of-development, and then added some of our own. We believe this is a relatively diverse representation of a country's social and economic state. \n",
    "\n",
    "Ultimately, the goal of the project is to give people insight into how social and economic differences between countries and continents influences energy consumption patterns. Climate change is one of the large problems that society is facing, and the fact that we have to think about the problem as a global phenomenon makes it tricky. The aim is therefore to serve information about the topic in an intuitive and interesting way, which will then allow people to have informed opinions about the matter and ultimately make better decisions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "## **2. Basic stats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data cleaning and preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is data cleaning and preprocessing. In this case, there is quite a lot. Both due to the fact that we use a large number of raw datasets, and that we have a large number of missing values. This latter point can be due to a whole host of reasons. First of all, some countries only start existing after 1965. Additionally, even though a country exists, it is not certain that the measurements have been made. This is particularly true for the African countries. Additionally, there missing values even after recording from a certain country has started. In order to enrich the data further, we will make use of linear interpolation and constant extrapolation (see respective section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import os\n",
    "from functools import reduce\n",
    "from country_list import countries_for_language\n",
    "import pycountry_convert as pc\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Load and join raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and join raw data, both energy and social data.\n",
    "\n",
    "- ***General comments***: From the webside \"ourworldindata.org\" we could see that data from 2020 was not completed so we discarded all data from 2020 to preserve accuracy of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to raw data files\n",
    "in_path = os.path.join(os.getcwd(), \"../data/raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and join energy consumption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get energy consumption files\n",
    "energy_data = [x for x in os.listdir(os.path.join(in_path,\"EnergyData\"))]\n",
    "\n",
    "# Annual and per capita consumption paths\n",
    "ac_paths = list(filter(lambda x: \"ac\" in x.lower(), energy_data))\n",
    "pcc_paths = list(filter(lambda x: \"pcc\" in x.lower(), energy_data))\n",
    "\n",
    "print(f\"Merging ac_dfs ({len(ac_paths)})\")\n",
    "ac_dfs = [\n",
    "    pd.read_csv(os.path.join(in_path, \"EnergyData\", filename))\n",
    "    for filename in ac_paths\n",
    "]\n",
    "for df in ac_dfs:\n",
    "    df.drop(\"Code\", inplace=True, axis=1)\n",
    "ac_final = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=[\"Entity\", \"Year\"], how=\"outer\"),\n",
    "    ac_dfs,\n",
    ")\n",
    "\n",
    "print(f\"Merging pcc_dfs ({len(pcc_paths)})\")\n",
    "pcc_dfs = [\n",
    "    pd.read_csv(os.path.join(in_path, \"EnergyData\", filename))\n",
    "    for filename in pcc_paths\n",
    "]\n",
    "for df in pcc_dfs:\n",
    "    df.drop(\"Code\", inplace=True, axis=1)\n",
    "pcc_final = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=[\"Entity\", \"Year\"], how=\"outer\"),\n",
    "    pcc_dfs,\n",
    ")\n",
    "\n",
    "# Annual consumption joined data\n",
    "ac_joined = ac_final[(ac_final.Year >= 1965) & (ac_final.Year < 2020)].reset_index().drop(columns=\"index\")\n",
    "# Per capita consumption joined data\n",
    "pcc_joined = pcc_final[(pcc_final.Year >= 1965) & (pcc_final.Year < 2020)].reset_index().drop(columns=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc_joined.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and join social data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get social data files\n",
    "socio_data = [x for x in os.listdir(os.path.join(in_path, \"Socio_eco_data\"))]\n",
    "socio_data = [x for x in socio_data if '.DS_Store' not in x]\n",
    "\n",
    "print(f\"Merging socio_dfs ({len(socio_data)})\")\n",
    "socio_dfs = [\n",
    "    pd.read_csv(os.path.join(in_path, \"Socio_eco_data\", filename))\n",
    "    for filename in socio_data\n",
    "]\n",
    "\n",
    "for df in socio_dfs:\n",
    "    if \"Code\" in df.columns:\n",
    "        df.drop(\"Code\", inplace=True, axis=1)\n",
    "    for col in df.columns:\n",
    "        if \"annotations\" in col:\n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "socio_data_final = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=[\"Entity\", \"Year\"], how=\"outer\"),\n",
    "    socio_dfs,\n",
    ")\n",
    "\n",
    "socio_data_joined = (\n",
    "    socio_data_final.sort_values([\"Entity\", \"Year\"])\n",
    "    .reset_index()\n",
    "    .drop(columns=\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socio_data_joined.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Simple preprocessing of joined data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing column names of social data and remove unwanted columns.\n",
    "\n",
    "- ***General comments***: From the above short outline of the social data dataframe we see that we had a multiple of columns from the raw data, which we did not wish to include in the analysis. These are therefore discarded and the remaining column names are furthermore changed to more suitable names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple preprocessing of social data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column renaming and removal of unwanted columns from social data\n",
    "df = socio_data_joined.copy()\n",
    "\n",
    "print(f\"Preprocessing data (Column renaming and extraction)\")\n",
    "\n",
    "cols = list(df.columns)\n",
    "for indx, col in enumerate(cols):\n",
    "    if \"Mortality rate, under-5\" in col:\n",
    "        cols[indx] = \"Child mortality rate (under 5 years - %)\"\n",
    "        df.loc[:, col] = (1.0 * df.loc[:, col]) / 1000.0\n",
    "    elif \"Human Dev\" in col:\n",
    "        cols[indx] = \"HDI\"\n",
    "    elif \"Access to basic drinking water\" == col:\n",
    "        cols[indx] = \"Basic_Drinking_Water_Rate\"\n",
    "    elif \"Access to basic sanitation services\" == col:\n",
    "        cols[indx] = \"Basic_Sanitation_Services_Rate\"\n",
    "    elif \"GDP per\" in col:\n",
    "        cols[indx] = \"GDP per capita ($)\"\n",
    "    elif \"Life expec\" in col:\n",
    "        cols[indx] = \"Life expectancy (years)\"\n",
    "    elif \"Entity\" in col:\n",
    "        cols[indx] = \"Entity\"\n",
    "    elif \"Year\" in col:\n",
    "        cols[indx] = \"Year\"\n",
    "    elif \"tertiary\" in col:\n",
    "        cols[indx] = \"Tertiary education (%)\"\n",
    "    elif \"Total tax revenue\" in col:\n",
    "        cols[indx] = \"Tax revenue of total GDP (%)\"\n",
    "    elif \"Individuals using the Internet\" in col:\n",
    "        cols[indx] = \"Internet users (%)\"\n",
    "    elif \"Population\" in col:\n",
    "        cols[indx] = \"Population\"\n",
    "    else:\n",
    "        cols[indx] = \"to_drop\"\n",
    "        df.drop(columns=col, inplace=True)\n",
    "\n",
    "df.columns = [col for col in cols if \"to_drop\" not in col]\n",
    "\n",
    "# Adjust for wrong unit for HDI\n",
    "df[\"HDI\"] = pd.to_numeric(df[\"HDI\"])\n",
    "for indx in range(len(df[\"HDI\"])):\n",
    "    if df.loc[indx, \"HDI\"] > 1:\n",
    "        df.loc[indx, \"HDI\"] = df.loc[indx, \"HDI\"] / 1000\n",
    "\n",
    "# Remove data prior to 1965, since we do not have any energy measures prior to this year.\n",
    "print(f\"Removing all rows with year prior to 1965\")\n",
    "df[\"Year\"] = pd.to_numeric(df[\"Year\"])\n",
    "df = df[df.Year >= 1965].reset_index().drop(columns=\"index\")\n",
    "df = df[df.Year < 2020].reset_index().drop(columns=\"index\")\n",
    "\n",
    "\n",
    "socio_data_joined = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fraction of nan values in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_frac_social = socio_data_joined.isnull().sum().sum()/(socio_data_joined.shape[0]*socio_data_joined.shape[1])\n",
    "nan_frac_energy = pcc_joined.isnull().sum().sum()/(pcc_joined.shape[0]*pcc_joined.shape[1])\n",
    "\n",
    "print(f'The fraction of nan values in the social data is currently: {nan_frac_social:.3f}')\n",
    "print(f'The fraction of nan values in the energy data is currently: {nan_frac_energy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Interpolating, extrapolating and splitting data on country/area level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial comments**\n",
    "\n",
    "*As seen in the above section, the amount of nan-values in our data is quite excessive at this point (about 43% for both datasets). During this section we manage to reduce the amount of non-nan values of the social data by about 7 percent points.*\n",
    "\n",
    "The main idea for this preprocessing step is to enrich our data, in order for us to better model and vizualize the data in a meaningfull way. We are aware that this introduces bias into the dataset, but we did feel like this was a better choice overall. Below is a short description of, how we did interpolate and extrapolate data from known data:\n",
    "\n",
    "\n",
    "- **Interpolating data:** We have done a linear interpolation for all datapoints in-between two known values wihtin the same *entity/country*. This means, that if we had a known value for *HDI* for Japan in 1985, no values for 1986-1992, and a known value for 1993, then we do linear interpolation of the datapoints in the timerange 1986-1992 based on 1985 and 1993.\n",
    "\n",
    "- **Extrapolating data:** Besides linear interpolation of data, we also did simple exterapolation of unknown datapoints. Specifically we choose to assume, that any given observed value would likely be somewhat the same 5 years into the future (and 5 years into the past). That is, we did constant extrapolation of unknown data. We are aware that linear extrapolation or auto-regressive extrapolation would have maybe yield a more fair approximation, but we still feel like the constant assumption was fair, given the few amount of years we did extrapolate.\n",
    "\n",
    "- **Splitting data on country/area level:** The raw data included both area specific and country specific measures. In the last part of this preprocessing steps, we did split the data into a country specific and area specific dataframe and discarded the area specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpolating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation function \n",
    "\n",
    "def interpolate_data(df_joined: pd.DataFrame):\n",
    "    print(f\"Interpolating data (linear)\")\n",
    "\n",
    "    df = df_joined.copy()\n",
    "    # Get shapes\n",
    "    N = df.shape[0]\n",
    "    cols_to_interp = df.columns[2:]\n",
    "\n",
    "    # Replace strings with nan\n",
    "    df.replace(\"nan\", np.nan, inplace=True)\n",
    "\n",
    "    # Force columns to be numeric (non entity columns)\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "    # Make pseudo-dataset for interpolating mask\n",
    "    df_interp_mask = df.copy()\n",
    "    df_interp_mask[cols_to_interp] = df_interp_mask[cols_to_interp] * 0\n",
    "    df_interp_mask.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "    # Go through each row - for missing years add rows and interpolate if possible\n",
    "    for i in range(1, N):\n",
    "        year_diff = df.Year[i] - df.Year[i - 1]\n",
    "        if (year_diff > 1) & (df.Entity[i] == df.Entity[i - 1]):\n",
    "            new_rows = np.array(\n",
    "                [\n",
    "                    [df.Entity[i]] * (year_diff - 1),\n",
    "                    np.arange(df.Year[i - 1] + 1, df.Year[i]),\n",
    "                ]\n",
    "            ).T\n",
    "\n",
    "            new_mask = np.array(\n",
    "                [\n",
    "                    [df.Entity[i]] * (year_diff - 1),\n",
    "                    np.arange(df.Year[i - 1] + 1, df.Year[i]),\n",
    "                ]\n",
    "            ).T\n",
    "\n",
    "            for col in cols_to_interp:\n",
    "                if (np.isnan(df.loc[i, col]) == False) & (\n",
    "                    np.isnan(df.loc[i - 1, col]) == False\n",
    "                ):\n",
    "                    new_rows = np.concatenate(\n",
    "                        [\n",
    "                            new_rows,\n",
    "                            np.linspace(\n",
    "                                df.loc[i - 1, col], df.loc[i, col], year_diff + 1\n",
    "                            )[1:-1].reshape((year_diff - 1, 1)),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    new_mask = np.concatenate(\n",
    "                        [\n",
    "                            new_mask,\n",
    "                            np.repeat(1, year_diff - 1).reshape((year_diff - 1, 1)),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                else:\n",
    "                    new_rows = np.concatenate(\n",
    "                        [\n",
    "                            new_rows,\n",
    "                            np.array(np.repeat(np.nan, year_diff - 1)).reshape(\n",
    "                                (year_diff - 1, 1)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    new_mask = np.concatenate(\n",
    "                        [\n",
    "                            new_mask,\n",
    "                            np.repeat(0, year_diff - 1).reshape((year_diff - 1, 1)),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "\n",
    "            new_rows = pd.DataFrame(new_rows, columns=df.columns)\n",
    "            new_rows.replace(\"nan\", np.nan, inplace=True)\n",
    "            for col in new_rows.columns[1:]:\n",
    "                new_rows[col] = pd.to_numeric(new_rows[col])\n",
    "\n",
    "            new_mask = pd.DataFrame(new_mask, columns=df.columns)\n",
    "            for col in new_mask.columns[1:]:\n",
    "                new_mask[col] = pd.to_numeric(new_mask[col])\n",
    "\n",
    "            df = (\n",
    "                pd.concat([df.iloc[:i], new_rows, df.iloc[i:]], axis=0)\n",
    "                .reset_index()\n",
    "                .drop(columns=\"index\")\n",
    "            )\n",
    "            df_interp_mask = (\n",
    "                pd.concat(\n",
    "                    [df_interp_mask.iloc[:i], new_mask, df_interp_mask.iloc[i:]], axis=0\n",
    "                )\n",
    "                .reset_index()\n",
    "                .drop(columns=\"index\")\n",
    "            )\n",
    "            N = N + year_diff - 1\n",
    "\n",
    "    # Go through each column and interpolate values if possible\n",
    "    for col in cols_to_interp:\n",
    "        for i in range(N - 2):\n",
    "            if not np.isnan(df.loc[i, col]):\n",
    "                indx_old_non_nan = i\n",
    "                while (df.Entity[i + 1] == df.Entity[i]) & (\n",
    "                    np.isnan(df.loc[i + 1, col])\n",
    "                ):\n",
    "                    i = i + 1\n",
    "                    if i == N - 1:\n",
    "                        break\n",
    "                if i == N - 1:\n",
    "                    break\n",
    "                if (df.Entity[i + 1] == df.Entity[i]) & (\n",
    "                    indx_old_non_nan != i\n",
    "                ):  # Non nan value followed by x nan values then non nan value (interpolation possible)\n",
    "                    i = i + 1\n",
    "                    df.loc[(indx_old_non_nan + 1) : (i - 1), col] = np.linspace(\n",
    "                        df.loc[indx_old_non_nan, col],\n",
    "                        df.loc[i, col],\n",
    "                        i - indx_old_non_nan + 1,\n",
    "                    )[1:-1]\n",
    "                    df_interp_mask.loc[\n",
    "                        (indx_old_non_nan + 1) : (i - 1), col\n",
    "                    ] = np.repeat(1, i - indx_old_non_nan - 1)\n",
    "                else:\n",
    "                    i = i + 1\n",
    "                    continue\n",
    "\n",
    "\n",
    "    return df, df_interp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate social and energy data\n",
    "socio_data_inter, socio_data_inter_mask = interpolate_data(socio_data_joined)\n",
    "pcc_inter, pcc_inter_mask = interpolate_data(pcc_joined)\n",
    "\n",
    "print(f'Number of non-null values in raw joined data (energy pc): {(pcc_joined.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in interpolated data (energy pc): {(pcc_inter.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in raw joined data (social): {(socio_data_joined.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in interpolated data (social): {(socio_data_inter.isnull()==False).sum().sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see an improvement in the amount of data, especially for the social data. This will surely help us build a more stable ML model (although more biased), and more smooth visualizations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extrapolating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolation function\n",
    "\n",
    "def extrapolate_data(df_inter: pd.DataFrame, df_inter_mask: pd.DataFrame, x_extrap: int):\n",
    "    df = df_inter.copy()\n",
    "    df_extrap_mask = df_inter_mask.copy()\n",
    "    \n",
    "    print(f\"Extrapolating data (max {x_extrap} years)\")\n",
    "\n",
    "    # Get shapes\n",
    "    N = df.shape[0]\n",
    "    cols_to_extrap = df.columns[2:]\n",
    "\n",
    "    if x_extrap >= 1:\n",
    "        for col in cols_to_extrap:\n",
    "            for i in range(1, N):\n",
    "                if (\n",
    "                    np.isnan(df.loc[i - 1, col])\n",
    "                    & (not np.isnan(df.loc[i, col]))\n",
    "                    & (df.loc[i - 1, \"Entity\"] == df.loc[i, \"Entity\"])\n",
    "                ):\n",
    "                    m = 1\n",
    "                    if i - m - 1 >= 0:\n",
    "                        while (\n",
    "                            np.isnan(df.loc[i - m - 1, col])\n",
    "                            & (df.loc[i - m - 1, \"Entity\"] == df.loc[i, \"Entity\"])\n",
    "                            & (m + 1 <= x_extrap)\n",
    "                        ):  # Go back a max of x_extrap years\n",
    "                            m = m + 1\n",
    "                            if i - m == 0:\n",
    "                                break\n",
    "\n",
    "                    df.loc[i - m : i - 1, col] = np.repeat(df.loc[i, col], m)\n",
    "                    df_extrap_mask.loc[i - m : i - 1, col] = np.repeat(2, m)\n",
    "\n",
    "            i = 0\n",
    "            while i < N - 1:\n",
    "                if (\n",
    "                    np.isnan(df.loc[i + 1, col])\n",
    "                    & (not np.isnan(df.loc[i, col]))\n",
    "                    & (df.loc[i + 1, \"Entity\"] == df.loc[i, \"Entity\"])\n",
    "                ):\n",
    "                    m = 1\n",
    "                    if i + m + 1 <= N - 1:\n",
    "                        while (\n",
    "                            np.isnan(df.loc[i + m + 1, col])\n",
    "                            & (df.loc[i + m + 1, \"Entity\"] == df.loc[i, \"Entity\"])\n",
    "                            & (m + 1 <= x_extrap)\n",
    "                        ):  # Go forward a max of x_extrap years\n",
    "                            m = m + 1\n",
    "                            if i + m == N - 1:\n",
    "                                break\n",
    "                    df.loc[i + 1 : i + m, col] = np.repeat(df.loc[i, col], m)\n",
    "                    df_extrap_mask.loc[i + 1 : i + m, col] = np.repeat(2, m)\n",
    "                    i += m + 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    return df, df_extrap_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolate social and energy data\n",
    "socio_data_extrap, socio_data_extrap_mask = extrapolate_data(socio_data_inter, socio_data_inter_mask, 5)\n",
    "pcc_extrap, pcc_extrap_mask = extrapolate_data(pcc_inter, pcc_inter_mask, 5)\n",
    "\n",
    "print(f'Number of non-null values in interpolated data (energy pc): {(pcc_inter.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in extrapolated data (energy pc): {(pcc_extrap.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in interpolated data (social): {(socio_data_inter.isnull()==False).sum().sum()}')\n",
    "print(f'Number of non-null values in extrapolated data (social): {(socio_data_extrap.isnull()==False).sum().sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We also see an improvement in the amount of data here, but only for the social data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fraction of nan values in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_frac_social = socio_data_extrap.isnull().sum().sum()/(socio_data_extrap.shape[0]*socio_data_extrap.shape[1])\n",
    "nan_frac_energy = pcc_extrap.isnull().sum().sum()/(pcc_extrap.shape[0]*pcc_extrap.shape[1])\n",
    "\n",
    "print(f'The fraction of nan values in the social data is currently: {nan_frac_social:.3f}')\n",
    "print(f'The fraction of nan values in the energy data is currently: {nan_frac_energy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the social data we can see that we have manage to reduce the amount of nan-values with about 7 percent points!\n",
    "\n",
    "**NB:** Be aware that it acutally seems like the fraction of nan-values for the energy data has become larger (which is also true), but the reason being is, that we were able to interpolate one of the columns in the energy data for a certain number of years, which leads to more nan values in the remaining columns (i.e. we add more rows to the data, with only one column being \"filled\" up with non-nan values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting data on country/area level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data on country and area level\n",
    "\n",
    "def split_data_country_area(df_extrap: pd.DataFrame, df_extrap_mask: pd.DataFrame):\n",
    "    df = df_extrap.copy()\n",
    "    df_mask = df_extrap_mask.copy()\n",
    "\n",
    "    print(f\"Splitting data into countries and areas\")\n",
    "\n",
    "    entities = pd.Series(df.Entity.unique(), dtype=\"string\")\n",
    "    entities.replace({\"&\": \"and\"}, inplace=True, regex=True)\n",
    "    countries = pd.Series(np.array(countries_for_language(\"en\"))[:, 1], dtype=\"string\")\n",
    "    countries.replace({\"&\": \"and\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Congo - Brazzaville\": \"Congo\"}, inplace=True, regex=True)\n",
    "    countries.replace(\n",
    "        {\"Congo - Kinshasa\": \"Democratic Republic of Congo\"}, inplace=True, regex=True\n",
    "    )\n",
    "    countries.replace({\"Côte\": \"Cote\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Curaçao\": \"Curacao\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Czechoslovakia\": \"Czechia\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Faroe Islands\": \"Faeroe Islands\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Hong Kong SAR China\": \"Hong Kong\"}, inplace=True, regex=True)\n",
    "    # countries.replace({\"Micronesia\":\"Micronesia (country)\"},inplace=True, regex=True)\n",
    "    countries.replace({\"Macao SAR China\": \"Macao\"}, inplace=True, regex=True)\n",
    "    countries.replace({\"Myanmar \\(Burma\\)\": \"Myanmar\"}, inplace=True, regex=True)\n",
    "    countries.replace(\n",
    "        {\"Palestinian Territories\": \"Palestine\"}, inplace=True, regex=True\n",
    "    )\n",
    "    countries.replace(\n",
    "        {\"São Tomé and Príncipe\": \"Sao Tome and Principe\"}, inplace=True, regex=True\n",
    "    )\n",
    "    countries.replace({\"St\\.\": \"Saint\"}, inplace=True, regex=True)\n",
    "    countries.replace(\n",
    "        {\"Saint Vincent and Grenadines\": \"Saint Vincent and the Grenadines\"},\n",
    "        inplace=True,\n",
    "        regex=True,\n",
    "    )\n",
    "    countries.replace(\n",
    "        {\"Saint Martin\": \"Saint Martin (French part)\"}, inplace=True, regex=True\n",
    "    )\n",
    "    countries.replace({\"Timor-Leste\": \"Timor\"}, inplace=True, regex=True)\n",
    "    countries.replace(\n",
    "        {\"Saint Barthélemy\": \"Saint Barthelemy\"}, inplace=True, regex=True\n",
    "    )\n",
    "    countries.replace(\n",
    "        {\"U\\.S\\. Virgin Islands\": \"United States Virgin Islands\"},\n",
    "        inplace=True,\n",
    "        regex=True,\n",
    "    )\n",
    "    countries.replace({\"Vatican City\": \"Vatican\"}, inplace=True, regex=True)\n",
    "    # countries.replace({\"Wallis and Futuna\":\"Wallis and Futuna Islands\"},inplace=True, regex=True)\n",
    "\n",
    "    idx_country = []\n",
    "    for ent in entities:\n",
    "        if any(ent == country for country in countries):\n",
    "            idx_country.append(True)\n",
    "        else:\n",
    "            idx_country.append(False)\n",
    "    map_country = dict(zip(entities, idx_country))\n",
    "\n",
    "    df_ent = df.Entity.astype(\"string\").replace({\"&\": \"and\"}, regex=True)\n",
    "\n",
    "    df_country = (\n",
    "        df.iloc[np.array(df_ent.map(map_country))].reset_index().drop(columns=\"index\")\n",
    "    )\n",
    "    df_area = (\n",
    "        df.iloc[np.array(df_ent.map(map_country) == False)]\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "    )\n",
    "    df_mask_country = (\n",
    "        df_mask.iloc[np.array(df_ent.map(map_country))]\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "    )\n",
    "    df_mask_area = (\n",
    "        df_mask.iloc[np.array(df_ent.map(map_country) == False)]\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "    )\n",
    "\n",
    "    # Add continent info and remove countries with no continent info\n",
    "    countries_code = pd.Series(np.array(countries_for_language('en'))[:,0],dtype=\"string\")\n",
    "    df_country['Continent'] = np.nan\n",
    "    for indx, ent in enumerate(df_country['Entity']):\n",
    "        try:\n",
    "            cont = pc.convert_continent_code_to_continent_name(\n",
    "            pc.country_alpha2_to_continent_code(countries_code.iloc[list(countries).index(ent)]))\n",
    "            df_country.loc[indx,'Continent'] = cont\n",
    "        except:\n",
    "            continue;\n",
    "\n",
    "    cols = df_country.columns[[0,-1]+list(np.arange(1,len(df_country.columns)-1,1))]\n",
    "    df_country = df_country[cols]\n",
    "    df_country = df_country[df_country['Continent'].notna()]\n",
    "\n",
    "\n",
    "    return df_country, df_mask_country, df_area, df_mask_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_data, _, _, _ = split_data_country_area(socio_data_extrap, socio_data_extrap_mask)\n",
    "energy_data, _, _, _ = split_data_country_area(pcc_extrap, pcc_extrap_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Datasets stats and exploratory analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sections describes the basics stats of the preprocessed energy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_describe = energy_data.describe()\n",
    "print(f'The number of observations in the energy dataset is {energy_data.shape[0]}, and we have {energy_data.shape[1]} features\\n - We might only use \"Energy' +\n",
    "       ' per capita (kWh)\" and \"Low-carbon energy per capita (kWh)\" [which we throughout our report will refer to as \"Renewable energy\"] for our analysis.')\n",
    "print(f'\\nFor most of the columns in this dataset we have only about {(1-energy_data[\"Fossil Fuels per capita (kWh)\"].isnull().sum().sum()/energy_data.shape[0])*100:.1f}%' + \n",
    "      f' percent non nan values, which results in {energy_describe.loc[\"count\",\"Coal per capita (kWh)\"]:.0f} observations')\n",
    "energy_describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect that one should pay attention to is that the distribution of energy consumption (even per capita, which accounts for the population of the country) is very scewed (see next section of our data analysis). Our main purpose with this analysis/article is to investigate the relationship between social metrics and types of energy consumed, but not how much energy the country is consuming in absolute terms. Thus modelling the absolute amount of \"Renewable energy consumption\" [\"Low-carbon energy per capita\"], might pose a significant issue, since we would mostly end up describing the the total amount of energy consumed, and not to which degree this would be renewable energies. \n",
    "Example countries per year:\n",
    "  - 1. country: 100 kWh per capita in total, from which 90 kWh comes from renewable energy (90%).\n",
    "  - 2. country: 300 kWh per capita, from which 100 kWh comes from renewable energy (33.3%).\n",
    "  \n",
    "In this regard we would think country 1 is doing a \"better\" job wrt. renewable energies than country 2, but modelling the absolute amount would tell you otherwise. If the total amount of energy per capita was almost the same for all countries, this would not have been a problem.\n",
    "We will thus construct the target variable as a fraction of renewable energy consumed per capita, compared to the total energy consumed per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Social data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes the basic stats of the preprocessed social data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_describe = social_data.describe()\n",
    "print(f'The number of observations in the energy dataset is {social_data.shape[0]}, and we have {social_data.shape[1]} features.\\n')\n",
    "print(f'As seen below, we have a different amount of observations (non-nan values) for all of our columns/features.'+\n",
    "      ' The column with the most observations is \"Population\", which has no nan-values. The column with fewest observatiopns is '+\n",
    "      f'\"Tax revenue of total GDP (%)\", which has {(1-social_data[\"Tax revenue of total GDP (%)\"].isnull().sum().sum()/social_data.shape[0])*100:.1f}%' + \n",
    "      f' percent non nan values, which results in {social_describe.loc[\"count\",\"Tax revenue of total GDP (%)\"]:.0f} observations')\n",
    "social_describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thing one might notice is that there seems to be at least one observation with \"Tertiary education (%)\" above 100%, which seems odd. Below we have outlined the observations,where the \"Tertiary education (%)\" meassure is above 100%. If it is not present in the above table, you might have to rerun the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_data[social_data[\"Tertiary education (%)\"]>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations with a \"Tertiary education (%)\" meassure above 100%. \n",
    "social_data = social_data[social_data[\"Tertiary education (%)\"]<=100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Contient/Country specific stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explores the number of observations we have for each country, continent, and number of distinct countries represented in each continent (comparison of both datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of datapoints for each country**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soc = social_data['Entity'].value_counts().reset_index().set_index('index')\n",
    "df_ene = energy_data['Entity'].value_counts().reset_index().set_index('index')\n",
    "\n",
    "df_test = df_soc.join(df_ene,how='outer',lsuffix='_count_social',rsuffix='_count_energy')\n",
    "df_test = df_test.sort_values('Entity_count_energy')\n",
    "df_test.plot(title='Number of observations per country', xlabel='country', ylabel='count', figsize=(15,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that we have a different amount of observations for each country. There are multiple reasons for this, sometimes we might simply not have observed anything for the country for a given year, and some countries does not exist anymore.\n",
    "\n",
    "We also see, that we do not have a great overlap of observation across countries for each of the two datasets. This means, that we will have to discard quite a lot of data, when we want to model the relationship between social metrics and fraction of renewable energy consumed (since the two datasets has to be joined on country/year combination)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of datapoints for each continent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soc = social_data['Continent'].value_counts().reset_index().set_index('index')\n",
    "df_ene = energy_data['Continent'].value_counts().reset_index().set_index('index')\n",
    "\n",
    "df_test = df_soc.join(df_ene,how='outer',lsuffix='_count_social',rsuffix='_count_energy')\n",
    "df_test = df_test.sort_values('Continent_count_energy')\n",
    "df_test.plot.barh(title='Number of observations per continent', xlabel='country', ylabel='count', figsize=(15,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like all continents are fairly well represented in both dataset (some contients does not have as many countries, so there will naturally be fewer observations for these)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of countries represented for each continent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soc = social_data[['Continent','Entity']].value_counts().reset_index()['Continent'].value_counts().reset_index().set_index('index')\n",
    "df_ene = energy_data[['Continent','Entity']].value_counts().reset_index()['Continent'].value_counts().reset_index().set_index('index')\n",
    "\n",
    "df_test = df_soc.join(df_ene,how='outer',lsuffix='_count_social',rsuffix='_count_energy')\n",
    "df_test = df_test.sort_values('Continent_count_energy')\n",
    "df_test.plot.barh(title='Number of distinct countries represented per continent', xlabel='country', ylabel='count', figsize=(15,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each continent also seems to be represented well by a fair amount of distinct countries. So in regards to this, we are not too worried with the data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "## **3. Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inital data analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scewness in energy data (absolute amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the basic stats of the energy data, we can see a large scewness in the energy per capita feature (and low-carbon energy), even if we only take into account data from 2000 and forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "energy_data_after_2000 = energy_data[energy_data['Year']>=2000]\n",
    "\n",
    "vars = energy_data.columns[3:]\n",
    "fig = make_subplots(rows=3, cols=int(len(vars)/3+1))\n",
    "for i, var in enumerate(vars):\n",
    "    fig.add_trace(\n",
    "        go.Box(y=energy_data_after_2000[var],\n",
    "        name=var),\n",
    "        row=int(np.floor(i/4+1)), col=i%4+1\n",
    "    )\n",
    "\n",
    "fig.update_traces(boxpoints=None, jitter=.4)\n",
    "fig.update_layout(\n",
    "    title=\"Boxplot of energy measures (only data after 2000)\",\n",
    "    legend_title=\"Energy features\",\n",
    ")\n",
    "fig.update_layout(height=600, width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, we have quite a lot of scewness in the data, when it comes to absolute measures. Let's have a look at the fraction of \"Renewable energy\" (\"Low-carbon energy per capita (kWh)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame({'Fraction renewable energy':energy_data_after_2000['Low-carbon energy per capita (kWh)']/energy_data_after_2000['Energy per capita (kWh)']})\n",
    "\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "fig.add_trace(\n",
    "        go.Box(y=y['Fraction renewable energy'],\n",
    "        name='Fraction renewable energy'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_traces(boxpoints=None, jitter=.4)\n",
    "fig.update_layout(\n",
    "    title=\"Boxplot of fraction of renewable energy (only data after 2000)\",\n",
    "    legend_title=\"Energy features\",\n",
    "    yaxis_title=\"Fraction\",\n",
    ")\n",
    "fig.update_layout(height=400, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is still scewed, but a lot less than than for the absolute values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Plotting the distribution of social data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_data_after_2000 = social_data[social_data['Year']>=2000]\n",
    "\n",
    "vars = social_data.columns[3:]\n",
    "fig = make_subplots(rows=2, cols=int(len(vars)/2))\n",
    "for i, var in enumerate(vars):\n",
    "    fig.add_trace(\n",
    "        go.Box(y=social_data_after_2000[var],\n",
    "        name=var),\n",
    "        row=int(np.floor(i/4+1)), col=i%4+1\n",
    "    )\n",
    "\n",
    "fig.update_traces(boxpoints=None, jitter=.4)\n",
    "fig.update_layout(\n",
    "    title=\"Boxplot of social measures (only data after 2000)\",\n",
    "    legend_title=\"Social features\",\n",
    ")\n",
    "fig.update_layout(height=500, width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our above data analysis, we do see quite a large spread in observed values for all the social measures, but it is not as scewed as the energy data. The feature with the most spread is, of course, the population - with countries as China and India making up most of the scweness. \n",
    "\n",
    "From these plots, however, we have no clear indication of, how all these measures develop over time (also energy). We will take a closer look at this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Plotting measures as function of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed data\n",
    "data_path = os.path.join(os.getcwd(), \"../data/processed\")\n",
    "data_pcc_country = pd.read_csv(\n",
    "    data_path + \"/pcc_energy_extrapolated_5_country.csv\", index_col=0\n",
    ")\n",
    "data_socio_country = pd.read_csv(data_path + \"/socio_extrapolated_5_country.csv\", index_col=0)\n",
    "\n",
    "data_pcc_country = data_pcc_country.set_index([\"Entity\", \"Continent\", \"Year\"])\n",
    "data_socio_country = data_socio_country.set_index([\"Entity\", \"Continent\", \"Year\"])\n",
    "\n",
    "df_social_energy = data_pcc_country.join(data_socio_country, how=\"outer\").reset_index()\n",
    "df_social_energy = (\n",
    "    df_social_energy.sort_values([\"Year\", \"Entity\"]).reset_index().drop(columns=\"index\")\n",
    ")\n",
    "df_social_energy[\"Fraction of Low-carbon energy per capita\"] = (\n",
    "    df_social_energy[\"Low-carbon energy per capita (kWh)\"]\n",
    "    / df_social_energy[\"Energy per capita (kWh)\"]\n",
    ")\n",
    "col_int = [\n",
    "    \"GDP per capita ($)\",\n",
    "    \"Child mortality rate (under 5 years - %)\",\n",
    "    \"HDI\",\n",
    "    \"Life expectancy (years)\",\n",
    "    \"Tertiary education (%)\",\n",
    "    \"Internet users (%)\",\n",
    "    \"Tax revenue of total GDP (%)\",\n",
    "]\n",
    "df_social_energy = df_social_energy.sort_values([\"Year\", \"Continent\", \"Entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for exploratory plots \n",
    "frac_global =  df_social_energy.groupby(\"Year\").mean()\n",
    "frac_global[\"Continent\"] = \"Global\"\n",
    "df_group_by_year_and_continent = df_social_energy.groupby([\"Year\", \"Continent\"]).mean()\n",
    "df_group_by_year_and_continent = df_group_by_year_and_continent.reset_index().append(frac_global.reset_index())\n",
    "plot_data = df_group_by_year_and_continent.reset_index()\n",
    "plot_data = plot_data.drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(plot_data, \n",
    "    x=\"Year\", y=\"Fraction of Low-carbon energy per capita\", \n",
    "    color=\"Continent\", width=800, height=500, \n",
    "    title=\"Fraction of renewable energy across time\", \n",
    "    labels={\"Year\": \"Year\", \"Fraction of Low-carbon energy per capita\": \"Fraction of renewable energy\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot, we see the fraction of renewable energy vs time. The fraction is defined as amount of renewable energy devided by total energy. We see several interesting things. Firstly, we see that Europe is steadily increasing thoughout the period. We also see that South America is higher than one might expect, which is something we will return to later. For Africa, we see that it drops some time during the 80s. This is due to the fact that some countries are not represented in the data up until that point. When they then enter, the average for Africa of course changes. We also see that most of the world population is below the continent mean, represented by Africa, Asia and North America. This plot is in some sense showing the information per capita. It is debatable if this is ideal, since, at the end of the day (no pun intended), the globe does not really care about per capita, only about total, global consumption. Ultimately, it is clear from the graph that the mean fraction is at least rising, although slowly. It is also clear that there is room for further analysis, since it looks like there are some interesting things going on. \n",
    "\n",
    "We can also take a peak at the social/economic data. Let's first look at tax revenue of total GDP vs time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(plot_data, x=\"Year\", y=\"Tax revenue of total GDP (%)\", \n",
    "    color=\"Continent\", width=800, height=500,\n",
    "    title=\"Tax revenue of total GDP across time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we notice is that the data does not start until 1975. We can also see that there is a very large difference between Europe and basically everyone else. This might be part of the explanation of what we observed in the previous plot, namely that Europe seemed to be the only country that were steadily increasing their fraction. If one wanted to speculate a bit, it is also possible that part of the explanation is the general focus on social-liberal democracy in Europe. \n",
    "\n",
    "We can also look at the Human Development Index (HDI), which is a one-number index that seeks to measure human development by aggregating a host of other metrics (source: https://hdr.undp.org/en/content/human-development-index-hdi). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(plot_data, x=\"Year\", y=\"HDI\", \n",
    "    color=\"Continent\", width=800, height=500,\n",
    "    title=\"Humen Development Index across time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the previous plot, we can see that this measure also only starts in 1975. Interestingly, we can also see that the shapes of the lines for each continent are very similar, the main difference being that they are shifted/translated. We can observe that it seems like the increase in HDI is slowing off towards the end. For highly developed continents (Europe in particular), it might make sense that it is slowing off. The index is on a scale from 0 to 1, so the absolute changes become harder the closer one comes to 1. On the other hand, the fact that we see the same slowing off for Africa might suggest that the observation could be due to global economic factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Machine Learning model (Partial Least Squares Algorithm)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short introduction to Partial Least Squares (PLS)** [*2*]\n",
    "\n",
    "In this section we will present a PLS machine learning model, which we will primarily use for encapturing underlying (latent) structures in the social data that correlates well with the target variable (*fraction of renewable energy consumed*). These are called PLS components and is a linear combination of all features. Some features are more *expressed* in a given PLS component than others. The PLS components are iteratively calculated as follows:\n",
    "\n",
    "$$\n",
    "\\max _{\\alpha} \\operatorname{Corr}^{2}(y, X \\alpha) \\operatorname{Var}(X \\alpha)\n",
    "$$\n",
    "subject to $\\|\\alpha\\|=1,\\,\\, \\alpha^{T} \\Sigma \\varphi_{I}=0, \\,\\,I=1, \\ldots, m-1$. \n",
    "\n",
    "In more common words the above describes, that the first PLS component will be that linear combination of features, which maximizes the variance in the feature space and correlation with the target varible (either postive or negative), simultaneously. The next PLS component should be perpendicular in feature space to the first one (constraint), while still trying to maximize same objective function. The third PLS component should be perpedicular to both the first and second PLS component, and so fourth. In essence, each PLS component describes a combination of features that \"ressonates\" either well or bad with the target variable. One might think of these latent variables as \"meta-types\" of countries.\n",
    "\n",
    "Further, as a main element in the PLS algorithm, we will use these latent variables as pseudo features for a linear regression model of the target variable. The key purpose for also training the linear regression model on these latent variables is to see, how well we can actually explain the target variable using this (sanity check that our PLS components are not completely useless).\n",
    "\n",
    "PLS is also good when there is multicollinearity in the features, which is definitely the case in this dataset (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see below that multiple of the features has a \n",
    "# very high correlation coefficient (around 0.9, -0.9), \n",
    "# and some pretty high (around 0.7, -0.7), which in all\n",
    "# indicates multicollinearity.\n",
    "social_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data comments**\n",
    "\n",
    "To account for the global change in climate focus over the years (i.e. not assumed to be captured by the social metrics), we have decided to only train the model on data from 2000 and forward. We base this year on the first *United Nations Climate Change conference* [*1*], which took place in 1995. We choose to skip the first 5 years of data, since it inherently does take time to take \"action on your words\". Also, as we were able to see from our above data analysis, each continent have very different mean renewable energy fraction. We imagine this might have something to do with different intercontinental political views (besides different social measures). To (somewhat) account for this \"latent\" information, we thus choose to include the continent of a country as a feature to our model. We used one-hot-encoding to encode the continents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### More data cleaning for preparing the data model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to train a PLS machine learning model on the data, we do need to extract a dataset containing the feature, *X*, and the corresponding target values, *y*. Each row in the respective datasets should match on *Entity/Country* and *Year* (i.e. the keys of each dataset). Below is a walkthrough of the data cleaning we did, in order for us to prepare the data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframes for ml preprocessing\n",
    "df_social = social_data.copy()\n",
    "df_energy = energy_data.copy()\n",
    "\n",
    "# Make sure both energy and social data keys (entity, year) are present in both datasets. Otherwise drop those records\n",
    "temp1 = list(zip(df_energy[[\"Entity\",\"Year\"]].value_counts().reset_index().drop(columns=0)['Entity'],\n",
    "                 df_energy[[\"Entity\",\"Year\"]].value_counts().reset_index().drop(columns=0)['Year']))\n",
    "temp2 = list(zip(df_social[[\"Entity\",\"Year\"]].value_counts().reset_index().drop(columns=0)['Entity'],\n",
    "                 df_social[[\"Entity\",\"Year\"]].value_counts().reset_index().drop(columns=0)['Year']))\n",
    "exclude_records1 = list(set(temp2) - set(temp1))\n",
    "exclude_records = list(set(temp1) - set(temp2)) \n",
    "\n",
    "# Comprehensive list of keys from both datasets, that is not a part the intersection\n",
    "exclude_records.extend(exclude_records1)\n",
    "\n",
    "# Extract keys\n",
    "list_energy = list(zip(df_energy['Entity'],df_energy['Year']))\n",
    "list_social = list(zip(df_social['Entity'],df_social['Year']))\n",
    "\n",
    "# Get list of all keys, that should not be excluded\n",
    "list_in_energy = [x not in exclude_records for x in list_energy]\n",
    "list_in_social = [x not in exclude_records for x in list_social]\n",
    "\n",
    "# Drop unwanted records and sort remaining data (so energy and social data align in index)\n",
    "df_energy = df_energy.iloc[list_in_energy,].reset_index().drop(columns='index')\n",
    "df_social = df_social.iloc[list_in_social,].reset_index().drop(columns='index')\n",
    "df_energy = df_energy.sort_values(['Entity','Year']).reset_index().drop(columns='index')\n",
    "df_social = df_social.sort_values(['Entity','Year']).reset_index().drop(columns='index')\n",
    "\n",
    "# Extract records, where we have no nan values combined between the two datasets (only for columns of interest)\n",
    "df_energy_sub = df_energy.iloc[:,[5,6]]\n",
    "df_social_sub = df_social\n",
    "\n",
    "energy_non_nan_indx = df_energy_sub.isna().sum(axis=1)==0\n",
    "social_non_nan_indx = df_social_sub.isna().sum(axis=1)==0\n",
    "\n",
    "indx_non_nan = energy_non_nan_indx & social_non_nan_indx\n",
    "\n",
    "df_energy_sub = df_energy_sub.iloc[np.array(indx_non_nan),].reset_index().drop(columns='index')\n",
    "df_social_sub = df_social_sub.iloc[np.array(indx_non_nan),].reset_index().drop(columns='index')\n",
    "\n",
    "# Due to numerical instability with too large values, we will work with log(population)\n",
    "df_social_sub.iloc[:,4] = df_social_sub.iloc[:,4]/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### Define target values and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target values and features\n",
    "X = df_social_sub\n",
    "y = df_energy_sub['Low-carbon energy per capita (kWh)']/df_energy_sub['Energy per capita (kWh)']\n",
    "\n",
    "# Select data \n",
    "Continent = 'all' #One-hot-encode continents feature\n",
    "year_max = 2019\n",
    "year_min = 2000\n",
    "\n",
    "X1 = X.copy()\n",
    "y1 = y.copy()\n",
    "\n",
    "if Continent == 'all':\n",
    "    X1 = X1[(X['Year'] <= year_max) & (X['Year'] >= year_min)]\n",
    "    y1 = y1[(X['Year'] <= year_max) & (X['Year'] >= year_min)]\n",
    "\n",
    "    X1 = pd.concat([X1,pd.get_dummies(X1.Continent, prefix='Continent')],axis=1)\n",
    "else:\n",
    "    X1 = X1[(X['Year'] <= year_max) & (X['Year'] >= year_min) & (X['Continent'] == Continent)]\n",
    "    y1 = y1[(X['Year'] <= year_max) & (X['Year'] >= year_min) & (X['Continent'] == Continent)]\n",
    "\n",
    "X1 = X1.reset_index().drop(columns='index')\n",
    "y1 = y1.reset_index().drop(columns='index')\n",
    "\n",
    "# Drop entity, years, and continent from features dataset\n",
    "X2 = X1.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### Find optimal number of PLS components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test set\n",
    "X_train,X_test,y_train,y_test = train_test_split(X2,y1,test_size=0.3,random_state=0) \n",
    "\n",
    "#define cross-validation method\n",
    "cv = RepeatedKFold(n_splits=20, n_repeats=3, random_state=1)\n",
    "\n",
    "mse = []\n",
    "n = len(X_train)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\") #Ignore runtime warning (divide by zero)\n",
    "\n",
    "    # Calculate MSE using cross-validation, adding one component at a time\n",
    "    for i in np.arange(1, 13):\n",
    "        pls = PLSRegression(n_components=i)\n",
    "        score = -1*model_selection.cross_val_score(pls, scale(X_train), y_train, cv=cv,\n",
    "                scoring='neg_mean_squared_error').mean()\n",
    "        mse.append(score)\n",
    "\n",
    "#plot test MSE vs. number of components\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.plot(np.arange(1,len(mse)+1,1),mse)\n",
    "plt.xlabel('Number of PLS Components')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE of as a function of the number of PLS components')\n",
    "print(f'The number of PLS components yielding the lowest MSE is: {np.argmin(mse)+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### Train and test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate MSE\n",
    "pls = PLSRegression(n_components=np.argmin(mse)+1)\n",
    "X_mean = X_train.mean()\n",
    "X_std = X_train.std()\n",
    "pls.fit(scale(X_train), y_train)\n",
    "\n",
    "X_test_std = (X_test-X_mean)/X_std\n",
    "print(f'The MSE of the test data is {mean_squared_error(y_test, pls.predict(X_test_std.to_numpy())):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and test values and sort them according to the predicted values\n",
    "y_test_dat = y_test.to_numpy().ravel()\n",
    "y_hat_dat = pls.predict(X_test_std.to_numpy()).ravel()\n",
    "\n",
    "sort_indx = np.argsort(y_hat_dat)\n",
    "y_test_sorted = y_test_dat[sort_indx]\n",
    "y_hat_sorted = y_hat_dat[sort_indx]\n",
    "\n",
    "# Df for test data and predictions\n",
    "test_data = pd.DataFrame({'y_test':y_test_sorted,'y_hat':y_hat_sorted, 'y_cont':X1['Continent'].iloc[X_test.index]})\n",
    "test_data = test_data.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(1,1,figsize=(15,7))\n",
    "sns.lineplot(data=test_data, x=test_data.index, y='y_test',ax=ax, linewidth=0.75, color='orange', linestyle='--')\n",
    "sns.scatterplot(data=test_data,x=test_data.index,y='y_test',ax=ax, color='orange', label='Test data')\n",
    "sns.lineplot(data=test_data, x=test_data.index, y='y_hat',ax=ax, color='steelblue', linewidth=3, label='Fitted data')\n",
    "ax.set_ylabel('Fraction of Renewable energy', fontsize=15)\n",
    "ax.set_xlabel('Test index', fontsize=15)\n",
    "fig.suptitle('Test data vs. fitted data', fontsize=15, y=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By no means a perfect model, but it does seem to capture the trend ok. We also have predicted values below zero, which indictaes that the model type is not perfect for this problem (one could have probably looked into *Generalized Linear Models* to solve this issue.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### Find R2 value of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSres = np.sum((test_data.y_test-test_data.y_hat)**2)\n",
    "SStot = np.sum((test_data.y_test-np.mean(test_data.y_test))**2)\n",
    "\n",
    "R2 = 1-SSres/SStot\n",
    "print(f'R2 value is: {R2:.3f}')\n",
    "print(f'The correaltion coefficient between the predicted and test values is: {np.corrcoef(test_data.y_test,test_data.y_hat)[1,0]:.3f}')\n",
    "print('\\n\\nThe loadings/correlations between the target variable and the PLS components are')\n",
    "print(pls.y_loadings_.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is in essence able to describe around 35% of the variance in the target value (so 35% of the variance in the countries different fraction of renewable energy consumption)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### Analysis of PLS components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PLS component loadings and enrich\n",
    "n_comp = 3\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\") #Ignore runtime warning (divide by zero)\n",
    "    loadings=pd.DataFrame(pls.x_loadings_.T)\n",
    "    loadings.columns = X2.columns\n",
    "\n",
    "    loadings_v = loadings.unstack().reset_index()\n",
    "    loadings_v.columns = ['Feature','PLS Component','Loading']\n",
    "\n",
    "    loadings_v = loadings_v[loadings_v['PLS Component'] < n_comp]\n",
    "    loadings_v['Continent'] = np.array(['Continent' in x for x in loadings_v['Feature']])\n",
    "    loadings_v['x'] = np.zeros(len(loadings_v['Feature']),)\n",
    "    loadings_v['PLS Component'] = loadings_v['PLS Component']+1\n",
    "\n",
    "# Extract social metrics and ohe continents from features\n",
    "social_feats = pd.Series([x for x in loadings_v['Feature'] if 'Continent' not in x]).unique()\n",
    "continents = pd.Series([x for x in loadings_v['Feature'] if 'Continent' in x]).unique()\n",
    "continents = np.array([x.split('_')[1] for x in continents])\n",
    "loadings_v = loadings_v.reset_index().drop(columns='index')\n",
    "\n",
    "for indx,feat in enumerate(loadings_v['Feature']):\n",
    "    if \"Continent\" in feat:\n",
    "        loadings_v.loc[indx,'Feature'] = feat.split('_')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a vizualization of the PLS component loadings (how much each feature is \"correlated\" with this component). Currently showing results for PLS component 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show PLS component number \"comp\"\n",
    "comp = 1\n",
    "\n",
    "colormap = {}\n",
    "for indx,i in enumerate(social_feats):\n",
    "    colormap[i] = px.colors.qualitative.Pastel[indx]\n",
    "for indx,i in enumerate(continents):\n",
    "    colormap[i] = px.colors.qualitative.Antique[indx]\n",
    "\n",
    "\n",
    "fig = px.bar(loadings_v[loadings_v['PLS Component'] == comp], x=\"x\", y=\"Loading\", color=\"Feature\", barmode='group',\n",
    "             facet_col='Continent', facet_col_wrap=1, color_discrete_map=colormap,\n",
    "             category_orders={'Continent':[True, False]},\n",
    "             labels={'x':''},\n",
    "             width=900, height=400,\n",
    "             facet_row_spacing = 0.125,\n",
    "             hover_name=\"Feature\",hover_data={\"Loading\": True,\n",
    "                                              \"Feature\": False,\n",
    "                                              \"Continent\": False,\n",
    "                                              \"x\":False,\n",
    "                                              \"PLS Component\":False})\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        showticklabels=False\n",
    "    ),\n",
    "    xaxis2 = dict(\n",
    "        showticklabels=False\n",
    "    ),\n",
    "    xaxis3 = dict(\n",
    "        showticklabels=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=40, r=100, t=40, b=40)\n",
    ")\n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.99,\n",
    "    xanchor=\"left\",\n",
    "    x=1.05\n",
    "))\n",
    "\n",
    "\"\"\"\n",
    "if comp == 1:\n",
    "    anno_text = str(f'         <b>PLS Component 1</b><br><br>'+\n",
    "                    f'The correlation coefficient <br>with the target variable is: <b>{pls.y_loadings_[0][0]:.3f}' + \n",
    "                    '</b><br><br>This PLS component seems to capture<br>'+\n",
    "                    'the highly developed <b>European</b><br>'+ \n",
    "                    'countries... Etc.<br>'+\n",
    "                    'Example: Iceland')\n",
    "elif comp == 2:\n",
    "    anno_text = str(f'         <b>PLS Component 2</b><br><br>'+\n",
    "                    f'The correlation coefficient <br>with the target variable is: <b>{pls.y_loadings_[0][1]:.3f}' + \n",
    "                    '</b><br><br>This PLS component seems to capture<br>'+\n",
    "                    'the poorly developed <b>South</b><br>'+ \n",
    "                    '<b>American</b> countries. (weird).. Etc.<br>'+\n",
    "                    'Example: Brazil')\n",
    "else:\n",
    "    anno_text = str(f'         <b>PLS Component 3</b><br><br>'+\n",
    "                    f'The correlation coefficient <br>with the target variable is: <b>{pls.y_loadings_[0][2]:.3f}' + \n",
    "                    '</b><br><br>This PLS component seems to capture<br>'+\n",
    "                    'the larger <b>Asian</b> countries. Etc<br>'+ \n",
    "                    'Example: China')\n",
    "\n",
    "fig.add_annotation(\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"left\",\n",
    "    yref = 'paper',\n",
    "    xref = 'paper',\n",
    "    x=1.625,\n",
    "    y=1,\n",
    "    text=anno_text,\n",
    "    font=dict(family=\"Courier New, monospace\", size=16, color=\"Black\"),\n",
    "    align=\"left\",\n",
    "    bordercolor=\"Black\",\n",
    "    borderwidth=1,\n",
    "    borderpad=4,\n",
    "    bgcolor=\"#EBECF0\",\n",
    "    showarrow=False,\n",
    "    opacity=0.8,\n",
    ")\n",
    "\"\"\"\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.replace(\"Continent=True\", \"<b>Continents</b>\")))\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.replace(\"Continent=False\", \"<b>Social metrics</b>\")))\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "## **4. Genre**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we made ues the article/magazine style as our data story type. \n",
    "\n",
    "**Visual narrative:**\n",
    "\n",
    "Visual Structuring:\n",
    "- We have made a lot of use of the Progress Bar / Timebar in order to show to progress of both energy consumption and social/economic measures over time. We are very much interested in seeing how things progress over time, so this is an ideal tool. \n",
    "\n",
    "Highlighting (NB):\n",
    "- We generally make use of color in order to direct the viewer's attention. We have a plot of the world that tracks fraction of renewables across time. The color grading goes from black (fraction of 0) to saturated green (fraction of 1), which should make the plot more intuitive. \n",
    "- We also make use of dropdown menues in order to enable the user to narrow down the information in the plots and focus on the particular things that that user finds interesting. \n",
    "- In general, we make quite a lot of use of motion, since, as mentioned earlier, we want to underpin that things are changing quite a lot over time. For instance, the fraction of renewables vs social/economic metric (the one with the blobs) has a lot of motion in it. \n",
    "\n",
    "Transition Guidance:\n",
    "- Since we decided to use a article/magazine style, we do not have transitions in a regular sense. The user will simply scroll down through the article, so the focus in terms of transition are on the ordering of the visuals and of the information. \n",
    "- Additionally, in order to within or between visual scenes, we use quite a few dropdown menues and check-boxes. This might not be transitions in a regular sense, but it is the closest we have to it. \n",
    "\n",
    "**Narrative Structure:**\n",
    "\n",
    "Ordering:\n",
    "- We make use of a linear ordering, suited to the magazine style. We do, however, also want to let the user do some exploration on their own, so we use dropdown menues and checkboxes. \n",
    "\n",
    "Interactivity:\n",
    "- Most figures have quite a lot of interaction. As mentioned earlier, we have both dropdown and checkboxes, and we also have information that pops up on hover-over. \n",
    "- Using the interactivity in the figures is very intuitive and straight-forward, so tutorials are not necessary. We have generally selected default views that make sense. In general, people are much more used to interaction than they might have been, so hand-holding is not as necessary. \n",
    "\n",
    "Messaging:\n",
    "-  In general, we use a lot of annotations in order to communicate information to the user. A great example is the figure for the PLS model, where one can toggle between the components that each have a specific textbox attached to it. \n",
    "- Since we use the article/magazine style, there is naturally a lot of messaging, which means that we also use tools like intoductory text and Summary / Synthesis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "## **5. Visualizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fraction of renewable energy across time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of this visual**:\n",
    "\n",
    "This is a rather simple line plot of the fraction of renewable energy vs time on a continent level. The fraction is calculated as the consumption stemming from renewables devided total energy consumption, thus giving a value between 0 and 1. The average of the continents is shown as global. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Why is this right for our story**:\n",
    "We use this as an \"introduction\" to the problem. It is a figure that is easy to understand and digest, while it still shows some interesting phenomena, which mandates further and deeper analysis. We use this graph only looking at energy consumption first, since our aim is to first show that some interesting things is going on here. We then go on to analyze it in combination with social/economic metrics. At the end of the day, is is the right figure since it tells the centerpoint of our problem in the simplest way possible. The idea is therefore to abstract away all the smaller details and look at overall trends. The smaller details can then be added in the following plots. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(plot_data, \n",
    "    x=\"Year\", y=\"Fraction of Low-carbon energy per capita\", \n",
    "    color=\"Continent\", width=800, height=500, \n",
    "    title=\"Fraction of renewable energy across time\", \n",
    "    labels={\"Year\": \"Year\", \"Fraction of Low-carbon energy per capita\": \"Fraction of renewable energy\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Energy measures across time (and fraction of renewables)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of this visual**:\n",
    "\n",
    "In this plot, we see a world map that varies with time. In the dropdown menu, one can choose different energy types. When you press play, a video starts showing the change of that particular energy type with time. The map is then colored at the country level, e.g. showing the coal consumption per capita across the globe. In order to see differences between countries, they are colored according to the heat-map to the right. As mentioned, this can be done for the different energy types, but it can also be done for fraction of renewable energy, which is our main point of interest in terms of energy in this project. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Why is this right for our story**:\n",
    "\n",
    "This is really useful for the story we want to tell, since we want to gain some insight into differences in energy consumption across the globe. Adding the time dimension gives some interesting and intuitive insight into the patterns and developments across time. Additionally, it is at the granularity level of the raw data, so this is way in which we can extract as much information as possible. The interactiveness is useful since it is a way for us to let the user do some exploration on our own and make their own observations. The plot is also nice since it really does show that there are quite a lot of difference in consumption from country to country, so it works as a nice transition into looking at the relationship between energy consumption and social/economic measures. Another nice biproduct of the figure is that we can easily see that we have limitations in terms of data: many of the countries are not represented until much later in time. Thus, in many ways, the figure is an integral part of our visual narrative, combining most of the elements mentioned in “5. Genre”, using Progress Bar / Timebar etc. It is also integrated in our narrative structure, where we here focus a lot on the interactivity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Relationship between energy and social/economic measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of this visual**:\n",
    "\n",
    "The main idea this plot is to allow the reader to explore the relationsships between the social/economic measures and the fraction of renewables one-by-one. This visual has the possibility to either reduce or add \"complexity\" in terms of information for the different meassures, in order to allow for both deeper and more shallow understanding of the relationship. We allow the user to explore the relationship across time interactively, this way both allowing the user to \"play around\" with the data in his/her own way, but also making it possible to capture time-variying relations. \n",
    "\n",
    "On the webside, we will provide the user with just a few \"interesting\" cases to study (in bullet-form), to help guide the user towards some of the more interesting relations that we found. This type of visual follows more of a visual narative, with little to no hand-holding.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Why is this right for our story**:\n",
    "\n",
    "We choose a plot like this, because the type of plot (together with frames/progression bar and trendlines), can capture quite a lot of information in a relatively simple figure. The relationship between multidimensional social data and the fraction of renewable is inherently very difficult to capture, without getting the sense of information overload. Thus we choose to allow for only single social measure comparisons, but with the addon of continent by color and population by size, which still gives the plots a natural look. \n",
    "\n",
    "Still, after exploring the social measures one-by-one, one might still get the feeling, that there is a lot of underlying structure, that we do not manage to visualize in plots like this. This naturally leads towards ML models, which can help us understand some of these more latent/complex structures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Short data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_energy = energy_data.set_index([\"Entity\", \"Continent\", \"Year\"])\n",
    "data_social = social_data.set_index([\"Entity\", \"Continent\", \"Year\"])\n",
    "\n",
    "df_social_energy = data_energy.join(data_social, how=\"outer\").reset_index()\n",
    "df_social_energy = (\n",
    "    df_social_energy.sort_values([\"Year\", \"Entity\"]).reset_index().drop(columns=\"index\")\n",
    ")\n",
    "df_social_energy[\"Fraction of Low-carbon energy per capita\"] = (\n",
    "    df_social_energy[\"Low-carbon energy per capita (kWh)\"]\n",
    "    / df_social_energy[\"Energy per capita (kWh)\"]\n",
    ")\n",
    "col_int = [\n",
    "    \"GDP per capita ($)\",\n",
    "    \"Child mortality rate (under 5 years - %)\",\n",
    "    \"HDI\",\n",
    "    \"Life expectancy (years)\",\n",
    "    \"Population\",\n",
    "    \"Tertiary education (%)\",\n",
    "    \"Internet users (%)\",\n",
    "    \"Tax revenue of total GDP (%)\",\n",
    "]\n",
    "df_social_energy = (\n",
    "    df_social_energy.sort_values([\"Year\", \"Continent\", \"Entity\"])\n",
    "    .reset_index()\n",
    "    .drop(columns=\"index\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#### Interactive on webside: #######\n",
    "####################################\n",
    "\n",
    "# Add/remove visual tools (Tick on/off bottoms)\n",
    "values = ['Continent', 'Scatter'] #Include scatter, continent, trendline by adding them to values\n",
    "\n",
    "#Choose comparison measure (Dropdown menu)\n",
    "x = \"HDI\" #choose between \n",
    "          # 'GDP per capita ($)', 'Child mortality rate (under 5 years - %)', 'Population', 'HDI',\n",
    "          # 'Life expectancy (years)', 'Tertiary education (%)','Internet users (%)',\n",
    "          # 'Tax revenue of total GDP (%)'\n",
    "\n",
    "####################################\n",
    "####################################\n",
    "\n",
    "y = \"Fraction of Low-carbon energy per capita\"\n",
    "df_int = (\n",
    "    df_social_energy.iloc[\n",
    "        np.sum(np.array(df_social_energy[[x, y]].isnull()) * 1.0, axis=1) == 0\n",
    "    ]\n",
    "    .reset_index()\n",
    "    .drop(columns=\"index\")\n",
    ")\n",
    "\n",
    "if x == \"Population\":\n",
    "    log_axis = True\n",
    "else:\n",
    "    log_axis = False\n",
    "\n",
    "for i in np.sort(df_int[\"Year\"].unique()):\n",
    "    if len(df_int[\"Continent\"][df_int[\"Year\"] == i].unique()) != 6:\n",
    "        df_int = df_int[df_int[\"Year\"] != i].reset_index().drop(columns=\"index\")\n",
    "    else:\n",
    "        break;\n",
    "\n",
    "if \"Continent\" in values:\n",
    "    color = \"Continent\"\n",
    "    trendline_color = None\n",
    "else:\n",
    "    color = None\n",
    "    trendline_color = \"Black\"\n",
    "\n",
    "if \"Scatter\" in values:\n",
    "    df_int[\"size\"] = df_int[\"Population\"] ** (1 / 2)  # \"Energy per capita (kWh)\"\n",
    "    size_max = 40\n",
    "else:\n",
    "    df_int[\"size\"] = df_int[x] * 0 + 0.001\n",
    "    size_max = 0.001\n",
    "\n",
    "if \"Trendline\" in values:\n",
    "    scope = \"trace\"\n",
    "    type = \"lowess\"\n",
    "    frac = 0.75\n",
    "else:\n",
    "    scope = \"trace\"\n",
    "    type = None\n",
    "    frac = None\n",
    "\n",
    "fig1 = px.scatter(\n",
    "    df_int,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    size=\"size\",\n",
    "    color=color,\n",
    "    animation_frame=\"Year\",\n",
    "    animation_group=\"Entity\",\n",
    "    log_x=log_axis,\n",
    "    size_max=size_max,\n",
    "    range_x=[np.min(df_int[x]), np.max(df_int[x]) * 1.1],\n",
    "    range_y=[-0.2, 1.2],\n",
    "    labels={y: \"Fraction of Renewable Energy \", x: x},\n",
    "    trendline_scope=scope,\n",
    "    trendline=type,\n",
    "    trendline_options=dict(frac=frac),\n",
    "    trendline_color_override=trendline_color,\n",
    "    hover_name=\"Entity\",\n",
    "    hover_data={\"Continent\": True, \"Year\": False, x: True, y: True, \"size\": False},\n",
    ")\n",
    "\n",
    "fig1.update_layout(margin={\"t\": 0, \"l\": 0, \"r\": 0, \"b\": 0})\n",
    "fig1.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 200\n",
    "fig1.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig1.add_hline(y=0, line_width=1, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "if (\"Trendline\" not in values) & (\"Scatter\" not in values):\n",
    "    fig1.add_annotation(\n",
    "        x=1 / 2 * (np.max(df_int[x]) * 1.1 - np.min(df_int[x])),\n",
    "        y=0.5,\n",
    "        text=\"Choose either Scatter or Trendline to show data\",\n",
    "        font=dict(family=\"Courier New, monospace\", size=16, color=\"Black\"),\n",
    "        align=\"center\",\n",
    "        bordercolor=\"Black\",\n",
    "        borderwidth=2,\n",
    "        borderpad=4,\n",
    "        bgcolor=\"#ADD8E6\",\n",
    "        showarrow=False,\n",
    "        opacity=0.8,\n",
    "    )\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PLS Component analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of this visual**:\n",
    "\n",
    "The main idea this plot is to provide the reader with an overview of the results from the ML model, presented in a way that both allows people with little understanding of the specific ML model to understand, what the results show, but also using some of the 'correct' terminology for the more ML advanced readers. We provide a small annotated description of PLS components we found, mainly translating (or help grasping) what these PLS components actually tells us. Here we want to show the main results of the ML model, thus only providing the visuals/annotations for the first 3 PLS components, both because these were the most significant ones, but also because we did not want to pass on too many conclusion for the reader (they might end up loosing track of what was really important). \n",
    "\n",
    "In this visual we do not allow the user to \"play around\" with the plot, but use more of a narative structure to really put emphasis on the results of our analysis.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Why is this right for our story**:\n",
    "\n",
    "This is the plot where we present the final results of our analysis, so this visual should, hopefully, close up the loose ends of the article. We thus should not allow for too much interpretation for the reader, but rather tell the reader, what he/she should take away from this - a lot of hand-holding. This visual is a way to provide the user with our results in both a visual and descriptive form, hopefully making it easier to grasp. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = 1\n",
    "colormap = {}\n",
    "for indx,i in enumerate(social_feats):\n",
    "    colormap[i] = px.colors.qualitative.Pastel[indx]\n",
    "for indx,i in enumerate(continents):\n",
    "    colormap[i] = px.colors.qualitative.Antique[indx]\n",
    "\n",
    "\n",
    "fig = px.bar(loadings_v[loadings_v['PLS Component'] == comp], x=\"x\", y=\"Loading\", color=\"Feature\", barmode='group',\n",
    "             facet_col='Continent', facet_col_wrap=1, color_discrete_map=colormap,\n",
    "             category_orders={'Continent':[True, False]},\n",
    "             labels={'x':''},\n",
    "             width=1300, height=500,\n",
    "             facet_row_spacing = 0.125,\n",
    "             hover_name=\"Feature\",hover_data={\"Loading\": True,\n",
    "                                              \"Feature\": False,\n",
    "                                              \"Continent\": False,\n",
    "                                              \"x\":False,\n",
    "                                              \"PLS Component\":False})\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        showticklabels=False\n",
    "    ),\n",
    "    xaxis2 = dict(\n",
    "        showticklabels=False\n",
    "    ),\n",
    "    xaxis3 = dict(\n",
    "        showticklabels=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=40, r=825, t=40, b=40)\n",
    ")\n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.99,\n",
    "    xanchor=\"left\",\n",
    "    x=1.05\n",
    "))\n",
    "\n",
    "if comp == 1:\n",
    "    anno_text = str(f'<b>PLS Component 1</b><br><br>'+\n",
    "                    f'The correlation coefficient <br>with the target variable is: <b>{pls.y_loadings_[0][0]:.3f}' + \n",
    "                    '</b><br><br>For the continents, we see that it<br>'+\n",
    "                    '<b>positively</b> correlates with <b>Europe</b>, while<br>'+ \n",
    "                    'it <b>negatively</b> correlates with <b>Asia</b>. For<br>'+\n",
    "                    'the social/economic metrics, we see<br>'+\n",
    "                    'high <b>positive</b> loadings in measures<br>'+\n",
    "                    'related do <b>high development</b>, while we<br>'+\n",
    "                    'have <b>negative</b> correlation for <b>child<br>'+\n",
    "                    'mortality</b> and <b>population</b>. It seems like<br>'+\n",
    "                    'this component captures <b>European</b> countries<br>'+\n",
    "                    'that are <b>highly developed</b> and puts it in<br>'+\n",
    "                    'opposition to <b>Asian</b> countries. <br><br>'+\n",
    "                    'An example of such a country could be <br>'+\n",
    "                    '<b>Iceland.</b>')\n",
    "\n",
    "elif comp == 2:\n",
    "    anno_text = str(f'<b>PLS Component 2</b><br><br>'+\n",
    "                    f'The correlation coefficient <br>with the target variable is: <b>{pls.y_loadings_[0][1]:.3f}' + \n",
    "                    '</b><br><br>Looking at the continents, we see that<br>'+\n",
    "                    'this component seems to represent <b>South<br>'+ \n",
    "                    'American</b> countries, putting it in<br>'+\n",
    "                    'opposition to <b>Asia</b>. For the social/economic<br>'+\n",
    "                    'measures, we see something interesting.<br>'+\n",
    "                    'Despite the component correlating<br>'+\n",
    "                    '<b>positively</b> with the response, we have<br>'+\n",
    "                    '<b>negative</b> correlations for many of the<br>'+\n",
    "                    'measures. This seems to suggest that<br>'+\n",
    "                    'the <b>South American</b> can have a relatively<br>'+\n",
    "                    '<b>high fraction of renewables</b> despite<br>'+\n",
    "                    'scoring low in the social/economic domain.<br><br>'+\n",
    "                    'A good example of this situation is <b>Brazil</b>.')\n",
    "\n",
    "else:\n",
    "    anno_text = str(f'<b>PLS Component 3</b><br><br>'+\n",
    "                    f'The correlation coefficient <br>with the target variable is: <b>{pls.y_loadings_[0][2]:.3f}' + \n",
    "                    '</b><br><br>Here, it looks like the component<br>'+\n",
    "                    'are very <b>populous</b> and are <b>relatively,<br>'+\n",
    "                    'developed</b> while also somewhat<br>' +\n",
    "                    'representing <b>Oceania</b>. It also seems<br>'+\n",
    "                    'like <b>African</b> countries (in particular)<br>' +\n",
    "                    'in this component show the opposite <br>'+\n",
    "                    'trend than the <b>Asian</b> ones.<br><br>'+\n",
    "                    'An example of a country that fits <br>'+\n",
    "                    'this description is <b>China</b>.')\n",
    "\n",
    "                    \n",
    "                      \n",
    "                      \n",
    "                       \n",
    "                     \n",
    "\n",
    "fig.add_annotation(\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"left\",\n",
    "    yref = 'paper',\n",
    "    xref = 'paper',\n",
    "    x=1.8,\n",
    "    y=1,\n",
    "    text=anno_text,\n",
    "    font=dict(family=\"Courier New, monospace\", size=16, color=\"Black\"),\n",
    "    align=\"left\",\n",
    "    bordercolor=\"Black\",\n",
    "    borderwidth=1,\n",
    "    borderpad=4,\n",
    "    bgcolor=\"#EBECF0\",\n",
    "    showarrow=False,\n",
    "    opacity=0.8,\n",
    ")\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.replace(\"Continent=True\", \"<b>Continents</b>\")))\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.replace(\"Continent=False\", \"<b>Social metrics</b>\")))\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "## **6. Discusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots we have created do a good job of telling the story that we want. The combination of an article with some interactive plots strikes a nice balance between finding a story that we want to tell, while also allowing the user to do some exploration on their own. The plots are in general also quite packed with information, but we think we have done a decent job of presenting that information in a way that makes it digestable. We have also managed to extract meaningful information about the problem, while not over-analyzing and jumping to conclusions that are not justified. At it's core, it is a very complex problem, and our analysis is off course not even close to definitive and complete. \n",
    "\n",
    "Things brings us to some of the short-comings of our creation. We got the impression from the litterature that there are other types of models that are often used for these kinds of problems, but we did not find the time to investiate and understand them. Thus, one avenue for improvement is looking more thoroughly into modelling approaches. One thing that is also clear from our analysis is that there is a lot of individuality from country to country. It can therefore be hard to really draw general conclusions. For example, Brazil does not score highly in most of the social/economic measures, but they have a relatively high fraction of renewables[*3*], which might be contrary to what our initial hypothesis was. On the other hand, we have country like the US, where the opposite is true. This goes to show that there are certain limitations to analyzing this problem quantitatively and that it is important to understand the specific circumstances of a country. One could argue that this could also be encoded, and thus modelled quantitatively, but that would require a much larger and more time-consuming analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "## **7. Contributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1. Motivation**: *ST37*\n",
    "- **2. Basic stats**\n",
    "    - **Data cleaning and preprocessing**: *ST150*\n",
    "    - **Datasets stats and exploratory analysis**: *ST150*\n",
    "- **3. Data analysis**\n",
    "    - **Initial Data analysis**: *ST37*, *ST150*\n",
    "    - **Machine learning algorithm (PLS)**: *ST150*\n",
    "- **4. Genre**: *ST37*\n",
    "- **5. Vizualizations**\n",
    "    - **Fraction of renewables across time**: *ST37*\n",
    "    - **Energy measures across time (and fraction of renewables)**: *ST173*\n",
    "    - **Relationship between energy and social/economic measures**: *ST150*\n",
    "    - **PLS Component analysis**: *ST150*, *ST37*\n",
    "- **6. Discussion**: *ST37*\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Homepage setup**: *ST173*\n",
    "- **Homepage**\n",
    "    - **Past, Present and future of energy**\n",
    "        - **Text**:\n",
    "        - **Visualization**: *ST173*\n",
    "    - **Exploring relationship between energy and social/econimic measures**\n",
    "        - **Text**:\n",
    "        - **Visualization**: *ST150*\n",
    "    - **Modelling relationship between energy and social/economic measures**\n",
    "        - **Text**:\n",
    "        - **Visualization**: *ST150*, *ST37*\n",
    "    - **From results to broader perspective**:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "## **8. References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [United Nations Climate Change conference *(1)*](https://en.wikipedia.org/wiki/United_Nations_Climate_Change_conference)\n",
    "- [Partial Least Squares *(2)*](https://en.wikipedia.org/wiki/Partial_least_squares_regression)\n",
    "- [Climate score card *(3)*](https://www.climatescorecard.org/2021/01/brazil-sources-45-of-its-energy-from-renewables/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f2d02c060971820c286dabf246074661ae9c6b11e8cd4009ace8b32c43e6634"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('socialboys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
